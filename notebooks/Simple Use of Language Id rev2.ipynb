{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a996d864-6a89-4513-95ba-2edb457d25be",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Utilisation simple des identifieurs de langues** (rev2) **avec les :**\n",
    ">### **- '*Sparse*' Bag Of Words**\n",
    ">### **- Tokenisations BERT ou Tiktoken**\n",
    ">### **- Classificateurs Naïve Bayes et Gradiant Boosting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e67500-a4c1-4d58-8f4a-1be20434be6d",
   "metadata": {},
   "source": [
    "#### **Choix du tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cb14523-04a8-424c-976a-d61585c0bbf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Choix de la Tokenisation (False = BERT, True Tiktoken)\n",
    "titoken_tokenization = True\n",
    "\n",
    "# Ce parametre permet éventuellement d'équilibrer de nombre de phrase par langue.\n",
    "# Si ce parametre est très grand, tout le corpus sera lu. \n",
    "nb_phrase_lang = 500000\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bad4ba-8678-41a4-9008-ab1915eddae6",
   "metadata": {},
   "source": [
    "#### **Lectures des phrases de \"sentences.csv\", et de leur étiquette \"Langue\" pour les langues sélectionnées**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "491d66b2-90bd-49e9-99cb-9601edf52a57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes de sentence.csv: 1750000\n",
      "Nombre de phrases par langue  ['eng', 'fra', 'deu', 'spa', 'ita'] : [350000, 350000, 350000, 350000, 350000]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lan_code</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eng</td>\n",
       "      <td>She is afraid of death.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ita</td>\n",
       "      <td>Indovina cosa scelgo io.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spa</td>\n",
       "      <td>¿Puedo ayudarlo? \"No, gracias. Solo estoy mira...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ita</td>\n",
       "      <td>Io non sono una fricchettona!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deu</td>\n",
       "      <td>Es sind schon fast 10 Jahre vergangen, aber du...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spa</td>\n",
       "      <td>Creía que me quería.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>eng</td>\n",
       "      <td>This school sets high moral standards for pupils.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>eng</td>\n",
       "      <td>Man is judged by his courage, woman by her charm.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fra</td>\n",
       "      <td>Je mange des pruneaux sucrés.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fra</td>\n",
       "      <td>J'ai écrit une chanson pour toi.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lan_code                                           sentence\n",
       "0      eng                            She is afraid of death.\n",
       "1      ita                           Indovina cosa scelgo io.\n",
       "2      spa  ¿Puedo ayudarlo? \"No, gracias. Solo estoy mira...\n",
       "3      ita                      Io non sono una fricchettona!\n",
       "4      deu  Es sind schon fast 10 Jahre vergangen, aber du...\n",
       "5      spa                               Creía que me quería.\n",
       "6      eng  This school sets high moral standards for pupils.\n",
       "7      eng  Man is judged by his courage, woman by her charm.\n",
       "8      fra                      Je mange des pruneaux sucrés.\n",
       "9      fra                   J'ai écrit une chanson pour toi."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lan_code</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1749990</th>\n",
       "      <td>deu</td>\n",
       "      <td>Es geschieht heutzutage ja so viel in unserer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749991</th>\n",
       "      <td>spa</td>\n",
       "      <td>El almuerzo está preparado.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749992</th>\n",
       "      <td>eng</td>\n",
       "      <td>I've seen enough.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749993</th>\n",
       "      <td>ita</td>\n",
       "      <td>Hanno accelerato il passo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749994</th>\n",
       "      <td>fra</td>\n",
       "      <td>Elle en pince pour ce garçon.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749995</th>\n",
       "      <td>deu</td>\n",
       "      <td>Wer von uns wünschte nicht manchmal, dass er d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749996</th>\n",
       "      <td>ita</td>\n",
       "      <td>No! Io odio i broccoli!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749997</th>\n",
       "      <td>fra</td>\n",
       "      <td>Tu seras tuée !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749998</th>\n",
       "      <td>fra</td>\n",
       "      <td>Tom aurait dû manger plus.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749999</th>\n",
       "      <td>eng</td>\n",
       "      <td>He took the video to a local TV station.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lan_code                                           sentence\n",
       "1749990      deu  Es geschieht heutzutage ja so viel in unserer ...\n",
       "1749991      spa                        El almuerzo está preparado.\n",
       "1749992      eng                                  I've seen enough.\n",
       "1749993      ita                         Hanno accelerato il passo.\n",
       "1749994      fra                      Elle en pince pour ce garçon.\n",
       "1749995      deu  Wer von uns wünschte nicht manchmal, dass er d...\n",
       "1749996      ita                            No! Io odio i broccoli!\n",
       "1749997      fra                                    Tu seras tuée !\n",
       "1749998      fra                         Tom aurait dû manger plus.\n",
       "1749999      eng           He took the video to a local TV station."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ouvrir le fichier d'entrée en mode lecture\n",
    "def create_lang_df(path):\n",
    "    df = pd.read_csv(path, index_col ='id')\n",
    "    return df\n",
    "\n",
    "df_big = create_lang_df('../data/multilingue/sentences.csv')\n",
    "lan_code = ['eng','fra','deu','spa','ita']\n",
    "df = pd.DataFrame(columns=df_big.columns)\n",
    "for i in range(len(lan_code)):\n",
    "    df= pd.concat([df, df_big[df_big['lan_code']==lan_code[i]].iloc[:nb_phrase_lang]])\n",
    "df = df.sample(frac=1, random_state=3).reset_index(drop=True)\n",
    "n_rows = len(df)\n",
    "print('Nombre de lignes de sentence.csv:',n_rows)\n",
    "nb_phrases_lang =[]\n",
    "for l in lan_code:\n",
    "    nb_phrases_lang.append(sum(df['lan_code']==l))\n",
    "print(\"Nombre de phrases par langue \",lan_code,\":\",nb_phrases_lang)\n",
    "display(df.head(10))\n",
    "display(df.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78bb348-f3c0-4455-b39e-a0b26527556a",
   "metadata": {},
   "source": [
    "#### **Selection du tokenizer** en fonction de la variable titoken_tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce260244-a177-43b5-accf-0564548c2fe5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Selection du tokenizer\n",
    "if titoken_tokenization:\n",
    "    import tiktoken\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "else:\n",
    "    from transformers import BertTokenizerFast\n",
    "    tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cb9b00-6cc3-47bc-afcf-b00fdef8738b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Préparation de la vectorisation par CountVectorizer**\n",
    "#### **Création de la fonction de création d'un Bags  Of Worlds pour les phrases à identifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df235a48-3ea5-47a7-af0a-f91fd20af84e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Les 2 fonctions suivantes sont nécéssaires afin de sérialiser ces parametre de CountVectorizer\n",
    "# et ainsi de sauvegarder le vectorizer pour un un usage ultérieur sans utiliser X_train pour  le réinitialiser\n",
    "def custom_tokenizer(text):\n",
    "    tokens = tokenizer.encode(text)  # Cela divise le texte en mots\n",
    "    return tokens\n",
    "\n",
    "def custom_preprocessor(text):\n",
    "    return text\n",
    "\n",
    "# CountVectorizer a une liste de phrase en entrée.\n",
    "# Cette fonction met les données d'entrée dans le bon format\n",
    "def format_to_vectorize(data):\n",
    "    X_tok = []\n",
    "    if \"DataFrame\" in str(type(data)):sentences = df.tolist()\n",
    "    elif \"str\" in str(type(data)):\n",
    "        sentences =[data]\n",
    "    else: sentences = data\n",
    "                          \n",
    "    for sentence in sentences:\n",
    "        X_tok.append(sentence) # ('¤'.join([tokenizer.decode([ids]) for ids in tokenizer.encode(sentence)])+'¤')\n",
    "    return X_tok\n",
    "\n",
    "def create_BOW(data):\n",
    "    global vectorizer\n",
    "    \n",
    "    X_tok = format_to_vectorize(data)\n",
    "    X = vectorizer.transform(X_tok)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf34d21-5bde-4d0f-a149-0ad5a48a39ed",
   "metadata": {},
   "source": [
    "#### **Chargement du vectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "773d2983-dd0a-450a-af97-a5d3034fe315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectorizer():\n",
    "    global dict_token, dict_ids, nb_token\n",
    "    \n",
    "    if titoken_tokenization: path = '../data/vectorizer_tiktoken.pkl'\n",
    "    else: path = '../data/vectorizer_BERT.pkl'\n",
    "    vectorizer = joblib.load(path)\n",
    "    dict_token = {tokenizer.decode([cle]): cle for cle, valeur in vectorizer.vocabulary_.items()}\n",
    "    dict_ids = {cle: tokenizer.decode([cle]) for cle, valeur in vectorizer.vocabulary_.items()} #dict_ids.items()}\n",
    "    nb_token = len(vectorizer.vocabulary_)\n",
    "    return vectorizer\n",
    "\n",
    "vectorizer = load_vectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a96239-98ab-47b0-acf9-1660e16fc9ab",
   "metadata": {},
   "source": [
    "#### **Choix du nom du fichier du classifieur sauvegardé**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acc778b1-231d-42c6-9218-144ee7e88da1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_file_name(titoken_tokenization, classifier):\n",
    "    if titoken_tokenization:\n",
    "        return \"id_lang_tiktoken_\"+classifier+\"_sparse.pkl\"\n",
    "    else:\n",
    "        return \"id_lang_BERT_\"+classifier+\"_sparse.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923233dd-f03a-400a-bc7c-b5fc4914ca8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Chargement du classificateur entrainé avec l'algorithme Naïve Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54b3449f-f635-402a-b3bf-5c163fadad52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "\n",
    "# Chargement du classificateur sauvé\n",
    "clf_nb = joblib.load(\"../data/\"+get_file_name(titoken_tokenization,\"nb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95249cde-cfc5-487c-b579-c590ee84ca42",
   "metadata": {},
   "source": [
    "#### **Chargement du classificateur entrainé avec l'algorithme Gradiant Boosting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbfa6b8c-172e-4cb7-ab10-20ed7154c245",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Chargement du classificateur sauvé\n",
    "clf_gb = joblib.load(\"../data/\"+get_file_name(titoken_tokenization,\"gb\"))\n",
    "######### dict_ids, decoded_keys = load_dict_token() ######### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de709b0-83c2-4b3f-a018-ea6c10531f99",
   "metadata": {},
   "source": [
    "#### **Definition de fonctions identificateur de langue avec Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83d8d36f-3f1c-49a8-8367-38158996865c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lang_id_nb(sentences):\n",
    "    if \"str\" in str(type(sentences)):\n",
    "        return clf_nb.predict(create_BOW(sentences))[0]\n",
    "    else: return clf_nb.predict(create_BOW(sentences))\n",
    "\n",
    "def lang_id_gb(sentences):\n",
    "    if \"str\" in str(type(sentences)):\n",
    "        return clf_gb.predict(create_BOW(sentences))[0]\n",
    "    else:\n",
    "        return clf_gb.predict(create_BOW(sentences))        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df82b0a7-3649-4c66-97d2-e38c52e2fbfc",
   "metadata": {},
   "source": [
    "#### **Exemples d'utilisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a73abd8-94fb-4ed5-8ad9-3308c95ca077",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\t lang\t Phrase\n",
      "0 -\t eng \t france is often snowy during spring , and it is relaxing in january .\n",
      "1 -\t fra \t elle adore les voitures très luxueuses, et toi ?\n",
      "2 -\t eng \t she loves very luxurious cars, don't you?\n",
      "3 -\t spa \t vamos a la playa\n",
      "4 -\t deu \t Ich heiße Keyne, und das ist wunderbar\n",
      "5 -\t e,f,d \t she loves you, mais elle te hait aussi, and das ist traurig\n",
      "6 -\t en \t I ate caviar\n",
      "7 -\t ita \t Vogliamo visitare il Colosseo e nuotare nel Tevere.\n",
      "8 -\t deu \t Drei von Toms Freunden besuchten Marias Feier.\n",
      "9 -\t spa \t ¿Qué piensas que hará ella?\n",
      "10 -\t deu \t Ich ziehe es vor, zu lesen, anstatt zu schreiben.\n",
      "11 -\t spa \t Ella le saluda cada mañana.\n",
      "12 -\t eng \t How many specimens can the public see at the Smithsonian Institution?\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Instanciation d'exemples\n",
    "\n",
    "sentence_no = random.sample(range(len(df)),5)\n",
    "\n",
    "exemples = [\"france is often snowy during spring , and it is relaxing in january .\",\n",
    "           \"elle adore les voitures très luxueuses, et toi ?\",\n",
    "           \"she loves very luxurious cars, don't you?\",\n",
    "           \"vamos a la playa\",\n",
    "           \"Ich heiße Keyne, und das ist wunderbar\",\n",
    "           \"she loves you, mais elle te hait aussi, and das ist traurig\", # Attention à cette phrase trilingue\n",
    "           \"I ate caviar\", \n",
    "           \"Vogliamo visitare il Colosseo e nuotare nel Tevere.\",\n",
    "            df['sentence'].iloc[sentence_no[0]],\n",
    "            df['sentence'].iloc[sentence_no[1]],\n",
    "            df['sentence'].iloc[sentence_no[2]],\n",
    "            df['sentence'].iloc[sentence_no[3]],\n",
    "            df['sentence'].iloc[sentence_no[4]],\n",
    "          ]\n",
    "lang_exemples = ['eng','fra','eng','spa','deu','e,f,d','en','ita',df['lan_code'].iloc[sentence_no[0]],df['lan_code'].iloc[sentence_no[1]],df['lan_code'].iloc[sentence_no[2]],\n",
    "                 df['lan_code'].iloc[sentence_no[3]],df['lan_code'].iloc[sentence_no[4]]]\n",
    "print('no\\t lang\\t Phrase')                            \n",
    "for i in range(len(exemples)):\n",
    "    print(i,'-\\t',lang_exemples[i],'\\t',exemples[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5b4a279-9aea-494e-b468-de0f024cd899",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langue réelle\tPréd. Naive B.\tPréd. Grad. B.\tPhrase\n",
      "eng\t\teng\t\teng\t\tfrance is often snowy during spring , and it is relaxing in january .\n",
      "fra\t\tfra\t\tfra\t\telle adore les voitures très luxueuses, et toi ?\n",
      "eng\t\teng\t\teng\t\tshe loves very luxurious cars, don't you?\n",
      "spa\t\tspa\t\tspa\t\tvamos a la playa\n",
      "deu\t\tdeu\t\tdeu\t\tIch heiße Keyne, und das ist wunderbar\n",
      "e,f,d\t\tfra\t\tdeu\t\tshe loves you, mais elle te hait aussi, and das ist traurig\n",
      "en\t\tita\t\teng\t\tI ate caviar\n",
      "ita\t\tita\t\tita\t\tVogliamo visitare il Colosseo e nuotare nel Tevere.\n",
      "deu\t\tdeu\t\tspa\t\tDrei von Toms Freunden besuchten Marias Feier.\n",
      "spa\t\tspa\t\tspa\t\t¿Qué piensas que hará ella?\n",
      "deu\t\tdeu\t\tdeu\t\tIch ziehe es vor, zu lesen, anstatt zu schreiben.\n",
      "spa\t\tspa\t\tspa\t\tElla le saluda cada mañana.\n",
      "eng\t\teng\t\teng\t\tHow many specimens can the public see at the Smithsonian Institution?\n"
     ]
    }
   ],
   "source": [
    "# Affichage des prédictions\n",
    "print(\"Langue réelle\\tPréd. Naive B.\\tPréd. Grad. B.\\tPhrase\")\n",
    "for i in range(len(exemples)):\n",
    "    print(lang_exemples[i]+'\\t\\t'+lang_id_nb(exemples[i])+'\\t\\t'+lang_id_gb(exemples[i])+'\\t\\t'+exemples[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed8d1c1-e5dd-4a17-8aee-1305fbf4c45a",
   "metadata": {},
   "source": [
    "> **Recherche des phrases mal classées par Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d458174a-c2e1-41e8-a84a-35584eaac751",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tN°Ligne\tL. réelle\tPréd. Naive B.\tPhrase\n",
      "1 \t 185 \t- ita \t\tspa \t\tMe lo traduci?  (proba=0.86)\n",
      "2 \t 209 \t- spa \t\tita \t\tAdmiro tu talento.  (proba=0.72)\n",
      "3 \t 921 \t- ita \t\tfra \t\tCe la caveremo.  (proba=0.79)\n",
      "4 \t 1743 \t- ita \t\tspa \t\tMarie era esigente.  (proba=0.82)\n",
      "5 \t 1841 \t- ita \t\tspa \t\tLo odio, lo odio, lo odio.  (proba=0.82)\n",
      "6 \t 1970 \t- spa \t\tita \t\tTom odia a Mary.  (proba=0.71)\n",
      "7 \t 2101 \t- eng \t\tita \t\tMagda marries a Spaniard.  (proba=0.67)\n",
      "8 \t 2365 \t- spa \t\tita \t\tAflójate la corbata.  (proba=0.60)\n",
      "9 \t 2659 \t- eng \t\tdeu \t\tTom is an unrepentant sinner.  (proba=0.97)\n",
      "10 \t 3111 \t- fra \t\tspa \t\tTom a visité Londres.  (proba=0.90)\n",
      "11 \t 3527 \t- fra \t\tita \t\tL'ennemi bombarda l'usine.  (proba=0.66)\n",
      "12 \t 3577 \t- ita \t\tspa \t\tVengo.  (proba=0.80)\n",
      "13 \t 3680 \t- ita \t\tspa \t\tEro a Boston.  (proba=0.76)\n",
      "14 \t 4607 \t- ita \t\tspa \t\tEra esigente.  (proba=0.89)\n",
      "15 \t 5604 \t- ita \t\tspa \t\tCrede a Babbo Natale.  (proba=0.64)\n",
      "16 \t 5973 \t- ita \t\tspa \t\tTocca a me?  (proba=0.83)\n",
      "17 \t 6035 \t- fra \t\tita \t\tVoici Tatoeba.  (proba=0.96)\n",
      "18 \t 6727 \t- ita \t\tspa \t\tEro a casa.  (proba=0.84)\n",
      "19 \t 7474 \t- ita \t\tspa \t\tAlzate la mano sinistra.  (proba=0.64)\n",
      "20 \t 7555 \t- ita \t\tspa \t\tLo vidi nudo.  (proba=0.88)\n",
      "21 \t 8103 \t- ita \t\tspa \t\tMary lo conosce.  (proba=0.97)\n",
      "22 \t 8790 \t- ita \t\tspa \t\tCostruiamo una yurta?  (proba=0.78)\n",
      "23 \t 9281 \t- spa \t\tita \t\tTomo un libro.  (proba=0.70)\n",
      "24 \t 9321 \t- ita \t\tspa \t\tTranquilla!  (proba=1.00)\n",
      "25 \t 9450 \t- ita \t\tspa \t\tMe la mandi.  (proba=0.60)\n",
      "26 \t 9471 \t- eng \t\tita \t\tVirtue and vice.  (proba=0.91)\n",
      "27 \t 9714 \t- eng \t\tfra \t\tMusic unites.  (proba=0.80)\n",
      "28 \t 9781 \t- spa \t\tita \t\tEra genial.  (proba=0.84)\n",
      "29 \t 10010 \t- spa \t\tita \t\tNo la soporto.  (proba=0.72)\n",
      "30 \t 10409 \t- ita \t\tspa \t\tPartecipa al contest!  (proba=0.57)\n"
     ]
    }
   ],
   "source": [
    "n_bad_max = 30\n",
    "n_bad = 0\n",
    "print(\"\\tN°Ligne\\tL. réelle\\tPréd. Naive B.\\tPhrase\")\n",
    "for i in range(len(df)):\n",
    "    if (n_bad<n_bad_max):\n",
    "        if (df['lan_code'].iloc[i] != lang_id_nb(df['sentence'].iloc[i])):\n",
    "            n_bad +=1\n",
    "            print(n_bad,'\\t',i,'\\t-',df['lan_code'].iloc[i],'\\t\\t'+lang_id_nb(df['sentence'].iloc[i]),'\\t\\t'+\n",
    "                  df['sentence'].iloc[i],\" (proba={:.2f}\".format(max(clf_nb.predict_proba(create_BOW([df['sentence'].iloc[i]]))[0]))+\")\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
