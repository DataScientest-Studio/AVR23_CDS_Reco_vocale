{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a996d864-6a89-4513-95ba-2edb457d25be",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Identification de 95 langues avec des :**\n",
    ">### **- '*Sparse*' Bag Of Words**\n",
    ">### **- Tokenisations BERT ou Tiktoken**\n",
    ">### **- CountVectorizer utilisant une tokenisation '*custom*'**\n",
    ">### **- Classificateurs Naïve Bayes et Gradiant Boosting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e67500-a4c1-4d58-8f4a-1be20434be6d",
   "metadata": {},
   "source": [
    "## **1 - Contruction des classificateurs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cb14523-04a8-424c-976a-d61585c0bbf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Choix de la Tokenisation (False = BERT, True Tiktoken)\n",
    "titoken_tokenization = True\n",
    "\n",
    "# Ce parametre permet éventuellement d'équilibrer de nombre de phrase par langue.\n",
    "# Si ce parametre est très grand, tout le corpus sera lu. \n",
    "nb_phrase_lang = 10000000\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bad4ba-8678-41a4-9008-ab1915eddae6",
   "metadata": {},
   "source": [
    "#### **Lectures des phrases de \"sentences.csv\", et de leur étiquette \"Langue\" pour les langues sélectionnées**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "491d66b2-90bd-49e9-99cb-9601edf52a57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes de sentence.csv: 10341812\n",
      "Nombre de langues à classer: 404\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lan_code</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ita</td>\n",
       "      <td>Il tuo futuro è pieno di possibilità.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fra</td>\n",
       "      <td>J'aimerais aller en France, un jour.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>epo</td>\n",
       "      <td>La polica enketo aperigis ilian sekretan vivon.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kab</td>\n",
       "      <td>Kullec ifukk yid-k.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hun</td>\n",
       "      <td>Több munkát nem tudok elvállalni.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10341807</th>\n",
       "      <td>deu</td>\n",
       "      <td>Wir werden das Problem nicht aufgreifen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10341808</th>\n",
       "      <td>fra</td>\n",
       "      <td>Je suis cuit !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10341809</th>\n",
       "      <td>kab</td>\n",
       "      <td>Isefk fell-ak a tregleḍ iɣis-a.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10341810</th>\n",
       "      <td>tok</td>\n",
       "      <td>o pana ala e moku tawa soweli tomo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10341811</th>\n",
       "      <td>hun</td>\n",
       "      <td>Máris unod magad?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10341812 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         lan_code                                         sentence\n",
       "0             ita            Il tuo futuro è pieno di possibilità.\n",
       "1             fra             J'aimerais aller en France, un jour.\n",
       "2             epo  La polica enketo aperigis ilian sekretan vivon.\n",
       "3             kab                              Kullec ifukk yid-k.\n",
       "4             hun                Több munkát nem tudok elvállalni.\n",
       "...           ...                                              ...\n",
       "10341807      deu         Wir werden das Problem nicht aufgreifen.\n",
       "10341808      fra                                   Je suis cuit !\n",
       "10341809      kab                  Isefk fell-ak a tregleḍ iɣis-a.\n",
       "10341810      tok              o pana ala e moku tawa soweli tomo.\n",
       "10341811      hun                                Máris unod magad?\n",
       "\n",
       "[10341812 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ouvrir le fichier d'entrée en mode lecture\n",
    "def create_lang_df(path):\n",
    "    df = pd.read_csv(path, index_col ='id')\n",
    "    return df\n",
    "\n",
    "df = create_lang_df('../data/multilingue/sentences-big.csv')\n",
    "lan_code = list(set(df['lan_code']))\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "n_rows = len(df)\n",
    "print('Nombre de lignes de sentence.csv:',n_rows)\n",
    "print('Nombre de langues à classer:',len(lan_code))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71dc774-4916-4658-b015-01594a8d003b",
   "metadata": {},
   "source": [
    "#### **Réalisation d'un jeu de données d'entrainement et de test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6dbac26f-2f26-4e3e-9685-1aa5914569b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lan_code</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cmn</td>\n",
       "      <td>咱们停止争吵和好吧。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fra</td>\n",
       "      <td>Sami a acheté une bouteille de vin.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fra</td>\n",
       "      <td>Tom a du mal à comprendre ce concept.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rus</td>\n",
       "      <td>Они идут друг за другом.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spa</td>\n",
       "      <td>Quisiera enviar este paquete a Japón.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9824715</th>\n",
       "      <td>rus</td>\n",
       "      <td>Давайте смотреть фактам в лицо!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9824717</th>\n",
       "      <td>jpn</td>\n",
       "      <td>私はそれについて全く知りません。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9824718</th>\n",
       "      <td>rus</td>\n",
       "      <td>Как я сегодня счастлив!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9824719</th>\n",
       "      <td>epo</td>\n",
       "      <td>Mi esperas, ke baldaŭ ni povu kunlabori.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9824720</th>\n",
       "      <td>rus</td>\n",
       "      <td>В последний раз я видел их в Албании.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9753920 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        lan_code                                  sentence\n",
       "0            cmn                                咱们停止争吵和好吧。\n",
       "1            fra       Sami a acheté une bouteille de vin.\n",
       "2            fra     Tom a du mal à comprendre ce concept.\n",
       "3            rus                  Они идут друг за другом.\n",
       "4            spa     Quisiera enviar este paquete a Japón.\n",
       "...          ...                                       ...\n",
       "9824715      rus           Давайте смотреть фактам в лицо!\n",
       "9824717      jpn                          私はそれについて全く知りません。\n",
       "9824718      rus                   Как я сегодня счастлив!\n",
       "9824719      epo  Mi esperas, ke baldaŭ ni povu kunlabori.\n",
       "9824720      rus     В последний раз я видел их в Албании.\n",
       "\n",
       "[9753920 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes par langue:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nb_phrases_lang</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lan_code</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>afr</th>\n",
       "      <td>4137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ara</th>\n",
       "      <td>38650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arq</th>\n",
       "      <td>2326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asm</th>\n",
       "      <td>3205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avk</th>\n",
       "      <td>4102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>war</th>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wuu</th>\n",
       "      <td>4757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yid</th>\n",
       "      <td>9603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yue</th>\n",
       "      <td>6230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zsm</th>\n",
       "      <td>6610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          nb_phrases_lang\n",
       "lan_code                 \n",
       "afr                  4137\n",
       "ara                 38650\n",
       "arq                  2326\n",
       "asm                  3205\n",
       "avk                  4102\n",
       "...                   ...\n",
       "war                  2025\n",
       "wuu                  4757\n",
       "yid                  9603\n",
       "yue                  6230\n",
       "zsm                  6610\n",
       "\n",
       "[95 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# créer 2 dataframes: 1 train (95% des phrases) et 1 test (5% des phrases)\n",
    "n_train = int(n_rows*0.95)\n",
    "df_train = df.iloc[:n_train].sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_test = df.iloc[n_train:].sample(frac=1, random_state=24).reset_index(drop=True)\n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "df_lan = pd.DataFrame(data= df.groupby('lan_code').size(), columns = ['nb_phrases_lang'] )\n",
    "\n",
    "# Filtrage des langues qui ont peu de phrases (>2000)\n",
    "df_lan = df_lan.loc[df_lan['nb_phrases_lang']>=2000]\n",
    "list_lan = list(set(df_lan.index))\n",
    "df_train = df_train[df_train['lan_code'].isin(list_lan)]\n",
    "df_test = df_test[df_test['lan_code'].isin(list_lan)]\n",
    "print('df_train:')\n",
    "display(df_train)\n",
    "print('Nombre de lignes par langue:')\n",
    "display(df_lan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78bb348-f3c0-4455-b39e-a0b26527556a",
   "metadata": {},
   "source": [
    "#### **Selection du tokenizer** en fonction de la variable titoken_tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce260244-a177-43b5-accf-0564548c2fe5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Selection du tokenizer\n",
    "if titoken_tokenization:\n",
    "    import tiktoken\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "else:\n",
    "    from transformers import BertTokenizerFast\n",
    "    tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1ccdf5-a5c4-4181-8b5a-22a3148d331a",
   "metadata": {},
   "source": [
    "#### **Préparation de la vectorisation par CountVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9be0a0e5-ac97-4d8a-93fb-bc4785e90808",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Les 2 fonctions suivantes sont nécéssaires afin de sérialiser ces parametre de CountVectorizer\n",
    "# et ainsi de sauvegarder le vectorizer pour un un usage ultérieur sans utiliser X_train pour  le réinitialiser\n",
    "def custom_tokenizer(text):\n",
    "    tokens = tokenizer.encode(text)  # Cela divise le texte en mots\n",
    "    return tokens\n",
    "\n",
    "def custom_preprocessor(text):\n",
    "    return text\n",
    "\n",
    "# CountVectorizer a une liste de phrase en entrée.\n",
    "# Cette fonction met les données d'entrée dans le bon format\n",
    "def format_to_vectorize(data):\n",
    "    X_tok = []\n",
    "    if \"DataFrame\" in str(type(data)):sentences = df.tolist()\n",
    "    elif \"str\" in str(type(data)):\n",
    "        sentences =[data]\n",
    "    else: sentences = data\n",
    "                          \n",
    "    for sentence in sentences:\n",
    "        X_tok.append(sentence) # ('¤'.join([tokenizer.decode([ids]) for ids in tokenizer.encode(sentence)])+'¤')\n",
    "    return X_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adba3da-befd-44cd-a602-13b87cb664e6",
   "metadata": {},
   "source": [
    "#### **Création de la fonction Vectorizer et de la fonction de création d'un Bags  Of Worlds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "029f861d-e36b-47e0-a43d-664e6f00720c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Création d'un vectorizer et du sparse BOW (X_train) avec le nombre d'apparitions\n",
    "global vectorizer, dict_ids, dict_token\n",
    "\n",
    "def create_vectorizer(X_train_tok):\n",
    "    global vectorizer, dict_ids, dict_token\n",
    "    \n",
    "    # token_pattern = r\"[a-zA-Z0-9\\s\\.\\,\\?\\:\\;]+\" \n",
    "    # vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=lambda x: tokenizer.encode(x), preprocessor=lambda x: x) #,token_pattern=token_pattern\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=custom_tokenizer, preprocessor=custom_preprocessor) #,token_pattern=token_pattern\n",
    "    vectorizer.fit(X_train_tok)\n",
    "    \n",
    "    # Création de dictionnaire des Token et des ids \n",
    "    dict_token = {tokenizer.decode([cle]): cle for cle, valeur in vectorizer.vocabulary_.items()}\n",
    "    dict_ids = {cle: tokenizer.decode([cle]) for cle, valeur in vectorizer.vocabulary_.items()} #dict_ids.items()}\n",
    "    return \n",
    "\n",
    "def create_BOW(data, vectorizer_to_create=False):\n",
    "    global vectorizer\n",
    "    \n",
    "    X_tok = format_to_vectorize(data)\n",
    "    if vectorizer_to_create:\n",
    "        create_vectorizer(X_tok)\n",
    "    X = vectorizer.transform(X_tok)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de78ee6-5749-4fe2-bbb4-6ae330e6415a",
   "metadata": {},
   "source": [
    "#### **Création du BOW Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "88502575-7222-4b4e-a615-e69b882830a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = create_BOW(df_train['sentence'], True)\n",
    "y_train = df_train['lan_code'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f4988d-49c4-43d1-8219-4971eea7d0ad",
   "metadata": {},
   "source": [
    "#### **Création du BOW Test**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a2e470a4-55c9-4516-8136-1331a5984720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test = create_BOW(df_test['sentence'])\n",
    "y_test = df_test['lan_code'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07204db8-0bd2-4742-b518-c12e5d28afe6",
   "metadata": {},
   "source": [
    "#### **Sauvegarde/Chargement du vectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04628919-c011-407a-8100-ff9bbd4b97e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definition de fonction de sauvegarde et chargement du dictionnaire des tokens utilisés\n",
    "def save_vectorizer(vectorizer):\n",
    "    if titoken_tokenization: path = '../data/vectorizer_tiktoken_big.pkl'\n",
    "    else: path = '../data/vectorizer_BERT.pkl' \n",
    "    joblib.dump(vectorizer, path)\n",
    "\n",
    "def load_vectorizer():\n",
    "    global dict_token, dict_ids, nb_token\n",
    "    \n",
    "    if titoken_tokenization: path = '../data/vectorizer_tiktoken_big.pkl'\n",
    "    else: path = '../data/vectorizer_BERT.pkl'\n",
    "    vectorizer = joblib.load(path)\n",
    "    dict_token = {tokenizer.decode([cle]): cle for cle, valeur in vectorizer.vocabulary_.items()}\n",
    "    dict_ids = {cle: tokenizer.decode([cle]) for cle, valeur in vectorizer.vocabulary_.items()} #dict_ids.items()}\n",
    "    nb_token = len(vectorizer.vocabulary_)\n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0c62c934-15d2-4a7e-9300-6cb288d3fcce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_vectorizer(vectorizer)\n",
    "\n",
    "vectorizer = load_vectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aad8bf-4b26-411e-84be-4280d9940bd5",
   "metadata": {},
   "source": [
    "#### **Definition d'une fonction ids->colonne de X_train et col->ids**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ded52ed4-1600-454a-83f6-196321e9a341",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ids2col(list_ids):\n",
    "    d = dict(vectorizer.vocabulary_.items())\n",
    "    list_col = []\n",
    "    for ids in list_ids:\n",
    "        list_col.append(d[ids])\n",
    "    return list_col\n",
    "\n",
    "def col2ids(list_col):\n",
    "    d = dict(vectorizer.vocabulary_.items())\n",
    "    list_ids = []\n",
    "    for col in list_col:\n",
    "        for ids, c in d.items():\n",
    "            if col==c:\n",
    "                list_ids.append(ids)\n",
    "                break\n",
    "    return list_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a164735-49e1-4944-a92a-2c4e375945c9",
   "metadata": {},
   "source": [
    "#### **Création d'un dictionnaire des tokens avec leur fréquence d'apparition dans Train**\n",
    "#### **Définition d'une liste de token trié par fréquence d'apparition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "508009b3-b248-46b3-9f5a-0c5c93e35e00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de tokens : 59122\n",
      "Liste des 50 tokens les plus fréquents: ['.', ',', '?', ' a', 'i', 'Tom', 'a', 'u', ' to', ' la', ' de', ' ', ' t', ' d', 'I', 'as', ' Tom', ' k', 'е', 'en', ' n', 'а', 'и', ' in', ' the', ' y', 'ו�', 'is', 'у', 'o', 'о', 'ом', ' с', 'er', ' в', 't', 'z', 'T', '!', ' that', 'ı', '。', ' i', \"'t\", 'י�', ' ad', 'in', ' ne', 'em', ' is']\n"
     ]
    }
   ],
   "source": [
    "freq = X_train.sum(axis=0)\n",
    "list_ids = col2ids(range(len(dict_ids)))\n",
    "list_token = [tokenizer.decode([ids]) for ids in list_ids]\n",
    "dict_freq = dict(zip(list_token,freq.tolist()[0]))\n",
    "dict_freq = dict(sorted(dict_freq.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "def ids2token(ids):\n",
    "    for token, valeur in dict_ids.items():\n",
    "        if valeur == ids:\n",
    "            token_trouvee = token\n",
    "            break\n",
    "    return token_trouvee\n",
    "\n",
    "nb_token = len(vectorizer.vocabulary_)\n",
    "print(\"Nombre de tokens :\",nb_token)\n",
    "\n",
    "# Définition d'une liste 'écrite' des tokens : decoded_keys\n",
    "# decoded_keys = [tokenizer.decode([ids]) for ids in [ids2token(key) for key in list(dict_freq.keys())]]\n",
    "decoded_keys = list(dict_freq.keys())\n",
    "print(\"Liste des 50 tokens les plus fréquents:\",decoded_keys[:50])\n",
    "\n",
    "# vocab_size = max(max(row) for row in X_train) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a96239-98ab-47b0-acf9-1660e16fc9ab",
   "metadata": {},
   "source": [
    "#### **Choix du nom du fichier de sauvegarde du classifieur**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "acc778b1-231d-42c6-9218-144ee7e88da1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_file_name(titoken_tokenization, classifier):\n",
    "    if titoken_tokenization:\n",
    "        return \"id_lang_tiktoken_\"+classifier+\"_sparse-big.pkl\"\n",
    "    else:\n",
    "        return \"id_lang_BERT_\"+classifier+\"_sparse-big.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923233dd-f03a-400a-bc7c-b5fc4914ca8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Création d'un classificateur avec l'algorithme Naïve Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c40a11f5-cf0d-4c5f-8ee1-29f6a05bf4df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/id_lang_tiktoken_nb_sparse-big.pkl']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import naive_bayes\n",
    "\n",
    "# On definit le classificateur Naive Bayes et on l'entraine sur les données Train:\n",
    "clf_nb = naive_bayes.MultinomialNB()  # BernoulliNB() # MultinomialNB() \n",
    "clf_nb.fit(X_train, y_train)\n",
    "\n",
    "# Save the model to a file\n",
    "joblib.dump(clf_nb, \"../data/\"+get_file_name(titoken_tokenization,\"nb\")) ######### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "54b3449f-f635-402a-b3bf-5c163fadad52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion du classificateur Naïve Bayes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Classe prédite</th>\n",
       "      <th>afr</th>\n",
       "      <th>ara</th>\n",
       "      <th>arq</th>\n",
       "      <th>asm</th>\n",
       "      <th>avk</th>\n",
       "      <th>aze</th>\n",
       "      <th>bel</th>\n",
       "      <th>ben</th>\n",
       "      <th>ber</th>\n",
       "      <th>bre</th>\n",
       "      <th>...</th>\n",
       "      <th>uig</th>\n",
       "      <th>ukr</th>\n",
       "      <th>urd</th>\n",
       "      <th>vie</th>\n",
       "      <th>vol</th>\n",
       "      <th>war</th>\n",
       "      <th>wuu</th>\n",
       "      <th>yid</th>\n",
       "      <th>yue</th>\n",
       "      <th>zsm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Classe réelle</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>afr</th>\n",
       "      <td>147</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ara</th>\n",
       "      <td>0</td>\n",
       "      <td>1901</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arq</th>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asm</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avk</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>war</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wuu</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yid</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>430</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yue</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zsm</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Classe prédite  afr   ara  arq  asm  avk  aze  bel  ben  ber  bre  ...  uig  \\\n",
       "Classe réelle                                                      ...        \n",
       "afr             147     0    0    0    0    0    0    0    0    0  ...    0   \n",
       "ara               0  1901    0    0    0    0    0    0    0    0  ...    0   \n",
       "arq               0    99    0    0    0    0    0    0    0    0  ...    1   \n",
       "asm               0     0    0  106    0    0    0   69    0    0  ...    0   \n",
       "avk               0     0    0    0  174    0    0    0    3    0  ...    0   \n",
       "...             ...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "war               0     0    0    0    0    0    0    0    0    0  ...    0   \n",
       "wuu               0     0    0    0    0    0    0    0    0    0  ...    0   \n",
       "yid               0     0    0    0    0    0    0    0    0    0  ...    0   \n",
       "yue               0     0    0    0    0    0    0    0    0    0  ...    0   \n",
       "zsm               0     7    3    0    0    0    0    0    0    0  ...    4   \n",
       "\n",
       "Classe prédite  ukr  urd  vie  vol  war  wuu  yid  yue  zsm  \n",
       "Classe réelle                                                \n",
       "afr               0    0    0    0    0    0    0    0    0  \n",
       "ara               0    0    0    0    0    0    0    0    0  \n",
       "arq               0    0    0    0    0    0    0    0    0  \n",
       "asm               0    0    0    0    0    0    0    0    0  \n",
       "avk               0    0    0    0    0    0    0    0    0  \n",
       "...             ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "war               0    0    0    0   73    0    0    0    0  \n",
       "wuu               0    0    0    0    0   75    0    2    0  \n",
       "yid               0    0    0    0    0    0  430    0    0  \n",
       "yue               0    0    0    0    0    0    0  203    0  \n",
       "zsm               0    0    0    0    0    0    0    0   95  \n",
       "\n",
       "[95 rows x 95 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Naïve Bayes = 0.960\n"
     ]
    }
   ],
   "source": [
    "# Chargement du classificateur sauvegardé\n",
    "clf_nb = joblib.load(\"../data/\"+get_file_name(titoken_tokenization,\"nb\"))\n",
    "\n",
    "# Verification de l'efficacité du classificateur grace à la matrice confusion\n",
    "y_pred = clf_nb.predict(X_test)\n",
    "accuracy_naive_bayes = accuracy_score(y_test, y_pred)\n",
    "print(\"Matrice de confusion du classificateur Naïve Bayes\")\n",
    "ct = pd.crosstab(y_test,y_pred,rownames=['Classe réelle'], colnames=['Classe prédite'])\n",
    "display(ct)\n",
    "print(\"Accuracy Naïve Bayes = {:.3f}\".format(accuracy_naive_bayes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aefaaa37-e415-43ee-906f-7aa80ba5d79d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Naïve Bayes sur eng, deu, fran ita, spa = 0.999\n"
     ]
    }
   ],
   "source": [
    "from sklearn import naive_bayes\n",
    "ct_weurope = ct[['eng','deu','fra','ita','spa']].loc[['eng','deu','fra','ita','spa']]\n",
    "s = ct_weurope.sum().sum()\n",
    "acc = (ct_weurope.loc['eng','eng']+ct_weurope.loc['deu','deu']+ct_weurope.loc['fra','fra']+ct_weurope.loc['ita','ita']+ct_weurope.loc['spa','spa'])/s\n",
    "print(\"Accuracy Naïve Bayes sur eng, deu, fran ita, spa = {:.3f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de709b0-83c2-4b3f-a018-ea6c10531f99",
   "metadata": {},
   "source": [
    "#### **Definition de fonction identificateur de langue**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "83d8d36f-3f1c-49a8-8367-38158996865c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lang_id_nb(sentences):\n",
    "    if \"str\" in str(type(sentences)):\n",
    "        return clf_nb.predict(create_BOW(sentences))[0]\n",
    "    else: return clf_nb.predict(create_BOW(sentences))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df82b0a7-3649-4c66-97d2-e38c52e2fbfc",
   "metadata": {},
   "source": [
    "#### **Exemples d'utilisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6a73abd8-94fb-4ed5-8ad9-3308c95ca077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instanciation d'un exemple\n",
    "exemples = [\"Er weiß überhaupt nichts über dieses Buch.\",                                                             # Phrase 0\n",
    "            \"france is often snowy during spring , and it is relaxing in january .\",                                  # Phrase 1\n",
    "           \"elle adore les voitures très luxueuses, et toi ?\",                                                        # Phrase 2\n",
    "           \"she loves very luxurious cars, don't you?\",                                                               # Phrase 3\n",
    "           \"vamos a la playa\",                                                                                        # Phrase 4\n",
    "           \"Ich heiße Keyne, und das ist wunderbar\",                                                                  # Phrase 5\n",
    "           \"she loves you much, mais elle te hait aussi and das ist traurig.\", # Attention à cette phrase trilingue   # Phrase 6\n",
    "           \"A crane raises heavy construction materials.\",                                                            # Phrase 7\n",
    "           \"Vogliamo visitare il Colosseo e nuotare nel Tevere.\"                                                      # Phrase 8\n",
    "          ]\n",
    "lang_exemples = ['deu','eng','fra','eng','spa','deu','en,fr,de','en','ita']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c5b4a279-9aea-494e-b468-de0f024cd899",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langue réelle                 : ['deu', 'eng', 'fra', 'eng', 'spa', 'deu', 'en,fr,de', 'en', 'ita']\n",
      "Prédictions Naive Bayes       : ['deu' 'eng' 'fra' 'eng' 'spa' 'deu' 'glg' 'eng' 'ita']\n"
     ]
    }
   ],
   "source": [
    "# Affichage des prédictions\n",
    "print('Langue réelle                 :',lang_exemples)\n",
    "print('Prédictions Naive Bayes       :',lang_id_nb(exemples))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed8d1c1-e5dd-4a17-8aee-1305fbf4c45a",
   "metadata": {},
   "source": [
    "> **Recherche des phrases mal classées par Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d458174a-c2e1-41e8-a84a-35584eaac751",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - No 92  - Réel: mkd  Prédit: rus      Не сум ни почнал.  (proba=0.78)\n",
      "2 - No 249  - Réel: eng  Prédit: gos      Haijo loves frikandels.  (proba=0.99)\n",
      "3 - No 297  - Réel: kab  Prédit: ber      Ameskar-a d win yessenqaden imidyaten imensayen.  (proba=1.00)\n",
      "4 - No 307  - Réel: gos  Prédit: nld      Doe hest dien buusdouk valen loaten.  (proba=0.88)\n",
      "5 - No 314  - Réel: grn  Prédit: tur      Oreroryete.  (proba=0.46)\n",
      "6 - No 339  - Réel: cmn  Prédit: kmr      法語包含二十六個字母: a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z。  (proba=1.00)\n",
      "7 - No 352  - Réel: ber  Prédit: kab      Tom yenǧeε Mary deg uεrur.  (proba=0.67)\n",
      "8 - No 354  - Réel: ile  Prédit: epo      Ili parla pri te, Mary.  (proba=0.94)\n",
      "9 - No 425  - Réel: kab  Prédit: ber      Ur lliɣ ara d tayemmat yelhan.  (proba=0.78)\n",
      "10 - No 438  - Réel: ukr  Prédit: rus      Том теж спить.  (proba=0.91)\n",
      "11 - No 465  - Réel: srp  Prédit: mkd      Она ће данас поподне да опере бицикл.  (proba=0.77)\n",
      "12 - No 501  - Réel: slk  Prédit: ces      Mám rád zemiakový šalát.  (proba=1.00)\n",
      "13 - No 528  - Réel: kab  Prédit: ber      Awal-nteɣ, nekkenti.  (proba=0.97)\n",
      "14 - No 588  - Réel: ber  Prédit: kab      Tizlit \"A Baba-inu Ba\" tettwasuqqel ɣer nnig n ɛecrin n tmeslayin.  (proba=0.83)\n",
      "15 - No 599  - Réel: nld  Prédit: tur      Tom gokt.  (proba=0.52)\n",
      "16 - No 664  - Réel: ber  Prédit: kab      Tcuffem iman-nwen am uzuxzux seg zzuxx.  (proba=0.76)\n",
      "17 - No 666  - Réel: kab  Prédit: ber      Netta ur yelli d bu tismin.  (proba=0.99)\n",
      "18 - No 702  - Réel: ber  Prédit: kab      Welyuc-iɣ mi ssemd-iɣ tira n Tabraṭt.  (proba=0.90)\n",
      "19 - No 710  - Réel: kab  Prédit: ber      Uriɣ.  (proba=0.51)\n",
      "20 - No 714  - Réel: lfn  Prédit: bul      Ла сениор дисиплина ки ел ама.  (proba=0.49)\n",
      "21 - No 759  - Réel: eng  Prédit: ina      Translations are rarely faithful. As the Italians say, \"traduttore, traditore\" (translator, traitor).  (proba=0.82)\n",
      "22 - No 765  - Réel: kab  Prédit: ber      Kra ur t-yuɣ.  (proba=0.66)\n",
      "23 - No 783  - Réel: ber  Prédit: kab      Igerdan ceffun.  (proba=0.61)\n",
      "24 - No 844  - Réel: bul  Prédit: rus      Тя участваше активно в женското либерално движение.  (proba=0.88)\n",
      "25 - No 845  - Réel: mkd  Prédit: rus      Полничок сум.  (proba=0.93)\n",
      "26 - No 853  - Réel: srp  Prédit: ukr      Цене су порасле.  (proba=0.42)\n",
      "27 - No 949  - Réel: bul  Prédit: rus      Предложението ти изглежда разумно.  (proba=0.97)\n",
      "28 - No 952  - Réel: nob  Prédit: dan      Firmaet går i rødt.  (proba=0.95)\n",
      "29 - No 955  - Réel: ber  Prédit: kab      Ur nhiṛeɣ ara deg Paris.  (proba=0.98)\n",
      "30 - No 977  - Réel: kab  Prédit: ber      S wacu i yettwassen Denis Papin?  (proba=0.91)\n"
     ]
    }
   ],
   "source": [
    "n_bad_max = 30\n",
    "n_bad = 0\n",
    "for i in range(len(y_test)):\n",
    "    if (y_test[i] != y_pred[i]) and (n_bad<n_bad_max):\n",
    "        n_bad +=1\n",
    "        print(n_bad,'- No',i,' - Réel:',y_test[i],' Prédit:',y_pred[i],'    ',df_test['sentence'].iloc[i],\" (proba={:.2f}\".format(max(clf_nb.predict_proba(create_BOW([df_test['sentence'].iloc[i]]))[0]))+\")\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
