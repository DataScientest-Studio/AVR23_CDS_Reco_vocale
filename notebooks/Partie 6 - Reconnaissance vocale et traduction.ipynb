{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e88ef37a-c412-44c4-a6b4-68566155ecb3",
   "metadata": {},
   "source": [
    "<center> <h1><bold>\n",
    "<hr style=\"border-width:2px;border-color:#1664c8\">\n",
    "Exploration de la reconnaissance vocale et de la traduction en temps réel\"</b><br>\n",
    "\n",
    "<hr style=\"border-width:2px;border-color:#1664c8\">\n",
    "</center> </h1></bold>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7037add-33d2-4852-9cf7-f008cb409582",
   "metadata": {},
   "source": [
    "Étape 1 : Configuration de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15b22ecc-dcfb-4281-ae84-1b2fea899fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (4.29.2)\n",
      "Requirement already satisfied: torch in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (2.0.1)\n",
      "Collecting sounddevice\n",
      "  Downloading sounddevice-0.4.6-py3-none-win_amd64.whl (199 kB)\n",
      "     ---------------------------------------- 0.0/199.7 kB ? eta -:--:--\n",
      "     ------------------------------------- 199.7/199.7 kB 11.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from transformers) (1.22.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from torch) (2.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from sounddevice) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from CFFI>=1.0->sounddevice) (2.21)\n",
      "Requirement already satisfied: fsspec in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Installing collected packages: sounddevice\n",
      "Successfully installed sounddevice-0.4.6\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers torch sounddevice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4275b0a6-2e7e-4267-88ec-844d6d54629f",
   "metadata": {},
   "source": [
    "Étape 2 : Importation des bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c991e7d-0022-47b5-9915-f4cff07ef36e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer, Wav2Vec2Processor, pipeline, MarianMTModel, MarianTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81962b39-2cf6-4cb0-9aa6-fff1a62e90ea",
   "metadata": {},
   "source": [
    "Étape 3 : Chargement du modèle Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc8f378c-743a-48b2-a4ab-1e7f85f6573c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Charger le modèle Whisper pour la reconnaissance vocale\n",
    "whisper_model_name = \"facebook/wav2vec2-base-960h\"\n",
    "tokenizer = Wav2Vec2Processor.from_pretrained(whisper_model_name)   # Wav2Vec2Tokenizer.from_pretrained(whisper_model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(whisper_model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b827f61f-8f02-4352-92ea-54f6deae2d86",
   "metadata": {},
   "source": [
    "Étape 4 : Fonction de reconnaissance vocale en temps réel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "64678450-6bbc-42a6-9680-3dba24de806b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_audio(audio):\n",
    "    # Normaliser les valeurs audio à l'intervalle [-1, 1]\n",
    "    audio = ((audio / np.max(np.abs(audio)))+1)/2\n",
    "    \n",
    "    # Convertir l'audio en mono (si ce n'est pas déjà fait)\n",
    "    if audio.shape[1] > 1:\n",
    "        audio = np.mean(audio, axis=1)\n",
    "    \n",
    "    # Rééchantillonner l'audio à 16 kHz si nécessaire\n",
    "    if len(audio) != 16000 * 5:  # Assurez-vous que l'audio a une durée de 5 secondes à 16 kHz\n",
    "        audio = sd.resample(audio, 16000 * 5)\n",
    "        \n",
    "    audio = audio.reshape((80000,)) #.astype(np.int16)\n",
    "    \n",
    "    # Conversion en tensor PyTorch\n",
    "    # inputs = tokenizer(audio, return_tensors=\"pt\", padding=\"longest\", return_attention_mask=True)\n",
    "    return audio # inputs\n",
    "\n",
    "\n",
    "def transcribe_audio(audio, tokenizer, model):\n",
    "    inputs = tokenizer(audio, return_tensors=\"pt\", padding=\"longest\", return_attention_mask=True)\n",
    "    print(\"inputs \",inputs)\n",
    "    # with torch.no_grad():\n",
    "    #     logits = model(input_values=inputs.input_values).logits\n",
    "    logits = model(input_values=inputs.input_values).logits\n",
    "    print(\"logits \", logits)\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    print(\"predicted_ids \", predicted_ids)\n",
    "    transcription = tokenizer.batch_decode(predicted_ids)\n",
    "    return transcription[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e0295a-daeb-422f-a684-5af275298b30",
   "metadata": {},
   "source": [
    "Étape 5 : Fonction de traduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71175d0e-6bf8-40d7-87ec-3ac23643850e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def translate_text(text, source_lang, target_lang):\n",
    "    # Charger le modèle de traduction MarianMT\n",
    "    model_name = f'Helsinki-NLP/opus-mt-fr-en'\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Tokeniser le texte\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # Traduire le texte\n",
    "    translation = model.generate(**inputs)\n",
    "    translated_text = tokenizer.batch_decode(translation, skip_special_tokens=True)\n",
    "    return translated_text[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6c3bab-f56f-4c44-9103-76185a786ede",
   "metadata": {},
   "source": [
    "Étape 6 : Reconnaissance vocale en temps réel et traduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d9f6aa8-ca20-4d62-adef-cb6bd577a6e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Démarrage de la reconnaissance vocale en temps réel...\n",
      "Texte reconnu : Bonjour\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olivi\\anaconda3\\envs\\DataScientest\\lib\\site-packages\\transformers\\generation\\utils.py:1346: UserWarning: Using `max_length`'s default (512) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traduction : Hello.\n",
      "Texte reconnu : Bonjour\n",
      "Traduction : Hello.\n",
      "Texte reconnu : Bonjour\n",
      "Traduction : Hello.\n",
      "Texte reconnu : Bonjour\n",
      "Traduction : Hello.\n",
      "Texte reconnu : Bonjour\n",
      "Traduction : Hello.\n",
      "Texte reconnu : Bonjour\n",
      "Traduction : Hello.\n",
      "Texte reconnu : Bonjour\n",
      "Traduction : Hello.\n",
      "Arrêt de la reconnaissance vocale en temps réel.\n"
     ]
    }
   ],
   "source": [
    "# def main():\n",
    "source_language = \"fr\"  # Changer en fonction de la langue détectée\n",
    "target_language = \"en\"  # Changer en fonction de la langue de traduction souhaitée\n",
    "\n",
    "print(\"Démarrage de la reconnaissance vocale en temps réel...\")\n",
    "while True:\n",
    "    try:\n",
    "        audio_input = sd.rec(frames=int(16000 * 5), samplerate=16000, channels=1, dtype=\"int16\")\n",
    "        \n",
    "        inputs = audio_input #preprocess_audio(audio_input)\n",
    "\n",
    "        sd.wait()\n",
    "        # text = transcribe_audio(inputs, tokenizer, model)\n",
    "        text='Bonjour'\n",
    "        print(f\"Texte reconnu : {text}\")\n",
    "\n",
    "        # Traduction du texte\n",
    "        translated_text = translate_text(text, source_language, target_language)\n",
    "        print(f\"Traduction : {translated_text}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Arrêt de la reconnaissance vocale en temps réel.\")\n",
    "        break\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "782ed297-8f95-4b17-a9ca-fc9408dd7317",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3,  6,  9, ..., 10, 11,  7], dtype=int16)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs\n",
    "audio_input.reshape((80000,)).astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "553bffdf-b13b-4283-aa2b-7427962555e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.50050356,  0.48841823,  0.01489303, ...,  0.48670919,\n",
       "        0.3836787 , -0.50608844])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_input = sd.rec(frames=int(16000 * 5), samplerate=16000, channels=1, dtype=\"int16\")\n",
    "audio_input = preprocess_audio(audio_input)\n",
    "audio_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2f472dd5-47c6-4bfb-9a90-81a183a3a010",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000 80000 <class 'numpy.ndarray'> <class 'numpy.float32'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.0000000e+00,  0.0000000e+00, -3.0517578e-05, ...,\n",
       "       -9.1552734e-05,  0.0000000e+00,  9.1552734e-05], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sampling_rate, len(audio_input), type(audio_input),type(audio_input[0][0]) )\n",
    "audio_input.reshape(80000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1da1fa42-da34-4e2b-b6b4-80e0fc68726e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000 93680 <class 'numpy.ndarray'> <class 'numpy.float64'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.00238037, 0.0020752 , 0.00198364, ..., 0.00042725, 0.00057983,\n",
       "       0.0010376 ])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sampling_rate, len(dataset[0][\"audio\"][\"array\"]), type(dataset[0][\"audio\"][\"array\"]), type(dataset[0][\"audio\"][\"array\"][0]))\n",
    "dataset[0][\"audio\"][\"array\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fad7a9b-ec36-4dc1-a638-681c68daa3a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Démarrage de la reconnaissance vocale en temps réel...\n",
      "Transcription:  AN ON MY MEMMI LETDY GOCONOIS ON I\n",
      "Loss:  30.59\n",
      "Texte reconnu : AN ON MY MEMMI LETDY GOCONOIS ON I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olivi\\anaconda3\\envs\\DataScientest\\lib\\site-packages\\transformers\\generation\\utils.py:1346: UserWarning: Using `max_length`'s default (512) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traduction : AN ON MY MEMOMI LETDY GOCONOIS ON I\n",
      "Transcription:  TO\n",
      "Loss:  7.58\n",
      "Texte reconnu : TO\n",
      "Traduction : TO\n",
      "Transcription:  ITI\n",
      "Loss:  12.54\n",
      "Texte reconnu : ITI\n",
      "Arrêt de la reconnaissance vocale en temps réel.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, Wav2Vec2ForCTC\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "source_language = \"fr\"  # Changer en fonction de la langue détectée\n",
    "target_language = \"en\"  # Changer en fonction de la langue de traduction souhaitée\n",
    "\n",
    " # load model and tokenizer\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\") # AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "sampling_rate = 16000\n",
    "# language_detection = pipeline('text-classification',model=\"papluca/xlm-roberta-base-language-detection\")\n",
    "\n",
    "print(\"Démarrage de la reconnaissance vocale en temps réel...\")\n",
    "while True:\n",
    "    try:\n",
    "\n",
    "        audio_input = sd.rec(frames=int(16000 * 5), samplerate=16000, channels=1)\n",
    "        sd.wait()\n",
    "        # language = language_detection(transcribe_audio(audio_input, processor, model))[0][\"language\"]\n",
    "        # print(f\"Langue détectée : {language}\")\n",
    "        inputs = processor(audio_input.reshape(80000,), sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "        sd.play(audio_input.reshape(80000,), 16000)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        # transcribe speech\n",
    "        transcription = processor.batch_decode(predicted_ids)\n",
    "        print(\"Transcription: \",transcription[0])\n",
    "        # 'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL'\n",
    "\n",
    "        inputs[\"labels\"] = processor(text=transcription[0], return_tensors=\"pt\").input_ids\n",
    "\n",
    "        # compute loss\n",
    "        loss = model(**inputs).loss\n",
    "        print(\"Loss: \",round(loss.item(), 2))\n",
    "        \n",
    "        sd.wait()\n",
    "        text = transcription[0]\n",
    "        print(f\"Texte reconnu : {text}\")\n",
    "\n",
    "        # Traduction du texte\n",
    "        translated_text = translate_text(text, source_language, target_language)\n",
    "        print(f\"Traduction : {translated_text}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Arrêt de la reconnaissance vocale en temps réel.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5800b120-34ed-4a54-a3bd-9e6bbdfbbb8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b366b352d98424db9fe0e4bd42ce91d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset librispeech_asr_demo (C:/Users/olivi/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_demo/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\olivi\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\70070395e1a266c43ac19a8da1134801a8d0725b13c03e6f199937974534a2a2\\\\dev_clean\\\\1272\\\\128104\\\\1272-128104-0000.flac'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m Wav2Vec2ForCTC\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/wav2vec2-base-960h\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# audio file is decoded on the fly\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m], sampling_rate\u001b[38;5;241m=\u001b[39msampling_rate, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     15\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DataScientest\\lib\\site-packages\\datasets\\arrow_dataset.py:2792\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2790\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[0;32m   2791\u001b[0m     \u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2792\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DataScientest\\lib\\site-packages\\datasets\\arrow_dataset.py:2777\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[1;34m(self, key, **kwargs)\u001b[0m\n\u001b[0;32m   2775\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m   2776\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 2777\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[0;32m   2779\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2780\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DataScientest\\lib\\site-packages\\datasets\\formatting\\formatting.py:629\u001b[0m, in \u001b[0;36mformat_table\u001b[1;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[0;32m    627\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DataScientest\\lib\\site-packages\\datasets\\formatting\\formatting.py:396\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[1;34m(self, pa_table, query_type)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable, query_type: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[RowFormat, ColumnFormat, BatchFormat]:\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 396\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    398\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DataScientest\\lib\\site-packages\\datasets\\formatting\\formatting.py:437\u001b[0m, in \u001b[0;36mPythonFormatter.format_row\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LazyRow(pa_table, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    436\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_arrow_extractor()\u001b[38;5;241m.\u001b[39mextract_row(pa_table)\n\u001b[1;32m--> 437\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_features_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m row\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DataScientest\\lib\\site-packages\\datasets\\formatting\\formatting.py:215\u001b[0m, in \u001b[0;36mPythonFeaturesDecoder.decode_row\u001b[1;34m(self, row)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_row\u001b[39m(\u001b[38;5;28mself\u001b[39m, row: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;28;01melse\u001b[39;00m row\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DataScientest\\lib\\site-packages\\datasets\\features\\features.py:1902\u001b[0m, in \u001b[0;36mFeatures.decode_example\u001b[1;34m(self, example, token_per_repo_id)\u001b[0m\n\u001b[0;32m   1888\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_example\u001b[39m(\u001b[38;5;28mself\u001b[39m, example: \u001b[38;5;28mdict\u001b[39m, token_per_repo_id: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1889\u001b[0m     \u001b[38;5;124;03m\"\"\"Decode example with custom feature decoding.\u001b[39;00m\n\u001b[0;32m   1890\u001b[0m \n\u001b[0;32m   1891\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;124;03m        `dict[str, Any]`\u001b[39;00m\n\u001b[0;32m   1900\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m   1903\u001b[0m         column_name: decode_nested_example(feature, value, token_per_repo_id\u001b[38;5;241m=\u001b[39mtoken_per_repo_id)\n\u001b[0;32m   1904\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_requires_decoding[column_name]\n\u001b[0;32m   1905\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m value\n\u001b[0;32m   1906\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m column_name, (feature, value) \u001b[38;5;129;01min\u001b[39;00m zip_dict(\n\u001b[0;32m   1907\u001b[0m             {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m example}, example\n\u001b[0;32m   1908\u001b[0m         )\n\u001b[0;32m   1909\u001b[0m     }\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DataScientest\\lib\\site-packages\\datasets\\features\\features.py:1903\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1888\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_example\u001b[39m(\u001b[38;5;28mself\u001b[39m, example: \u001b[38;5;28mdict\u001b[39m, token_per_repo_id: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1889\u001b[0m     \u001b[38;5;124;03m\"\"\"Decode example with custom feature decoding.\u001b[39;00m\n\u001b[0;32m   1890\u001b[0m \n\u001b[0;32m   1891\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;124;03m        `dict[str, Any]`\u001b[39;00m\n\u001b[0;32m   1900\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m-> 1903\u001b[0m         column_name: \u001b[43mdecode_nested_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1904\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_requires_decoding[column_name]\n\u001b[0;32m   1905\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m value\n\u001b[0;32m   1906\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m column_name, (feature, value) \u001b[38;5;129;01min\u001b[39;00m zip_dict(\n\u001b[0;32m   1907\u001b[0m             {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m example}, example\n\u001b[0;32m   1908\u001b[0m         )\n\u001b[0;32m   1909\u001b[0m     }\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DataScientest\\lib\\site-packages\\datasets\\features\\features.py:1325\u001b[0m, in \u001b[0;36mdecode_nested_example\u001b[1;34m(schema, obj, token_per_repo_id)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (Audio, Image)):\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;66;03m# we pass the token to read and decode files from private repositories in streaming mode\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m schema\u001b[38;5;241m.\u001b[39mdecode:\n\u001b[1;32m-> 1325\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DataScientest\\lib\\site-packages\\datasets\\features\\audio.py:181\u001b[0m, in \u001b[0;36mAudio.decode_example\u001b[1;34m(self, value, token_per_repo_id)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mKeyError\u001b[39;00m):\n\u001b[0;32m    179\u001b[0m         use_auth_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mxopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    182\u001b[0m         array, sampling_rate \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mread(f)\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DataScientest\\lib\\site-packages\\datasets\\download\\streaming_download_manager.py:493\u001b[0m, in \u001b[0;36mxopen\u001b[1;34m(file, mode, use_auth_token, *args, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m main_hop, \u001b[38;5;241m*\u001b[39mrest_hops \u001b[38;5;241m=\u001b[39m file_str\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m::\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_local_path(main_hop):\n\u001b[1;32m--> 493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(main_hop, mode, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    494\u001b[0m \u001b[38;5;66;03m# add headers and cookies for authentication on the HF Hub and for Google Drive\u001b[39;00m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rest_hops \u001b[38;5;129;01mand\u001b[39;00m (main_hop\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m main_hop\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\olivi\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\70070395e1a266c43ac19a8da1134801a8d0725b13c03e6f199937974534a2a2\\\\dev_clean\\\\1272\\\\128104\\\\1272-128104-0000.flac'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, Wav2Vec2ForCTC\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "      \n",
    "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
    "dataset = dataset.sort(\"id\")\n",
    "sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
    "      \n",
    "processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "      \n",
    "# audio file is decoded on the fly\n",
    "inputs = processor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "       \n",
    " # transcribe speech\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "print(transcription[0])\n",
    "# 'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL'\n",
    "      \n",
    "inputs[\"labels\"] = processor(text=dataset[0][\"text\"], return_tensors=\"pt\").input_ids\n",
    "      \n",
    "# compute loss\n",
    "loss = model(**inputs).loss\n",
    "round(loss.item(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68c73f4-da8d-4608-ab81-3108c5d5b12d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d09a8217-147a-4018-abce-114b3ae2a2cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soundfile in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (0.12.1)\n",
      "Requirement already satisfied: librosa in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (0.10.1)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from soundfile) (1.15.1)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from librosa) (1.2.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from librosa) (1.1.3)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from librosa) (1.11.1)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from librosa) (0.3.6)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from librosa) (1.0.3)\n",
      "Requirement already satisfied: pooch>=1.0 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from librosa) (1.7.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from librosa) (0.57.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from librosa) (1.22.3)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from librosa) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from librosa) (4.5.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from librosa) (0.3)\n",
      "Requirement already satisfied: pycparser in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from numba>=0.51.0->librosa) (0.40.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from pooch>=1.0->librosa) (2.28.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from pooch>=1.0->librosa) (2.5.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from pooch>=1.0->librosa) (23.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from scikit-learn>=0.20.0->librosa) (3.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\olivi\\anaconda3\\envs\\datascientest\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2023.5.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Found cached dataset librispeech_asr_dummy (C:/Users/olivi/.cache/huggingface/datasets/patrickvonplaten___librispeech_asr_dummy/clean/2.1.0/f2c70a4d03ab4410954901bde48c54b85ca1b7f9bf7d616e7e2a72b5ee6ddbfc)\n",
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "len() of unsized object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m ds \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatrickvonplaten/librispeech_asr_dummy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# tokenize\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m input_values \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlongest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39minput_values  \u001b[38;5;66;03m# Batch size 1\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# retrieve logits\u001b[39;00m\n\u001b[0;32m     17\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(input_values)\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DataScientest\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:93\u001b[0m, in \u001b[0;36mWav2Vec2Processor.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to specify either an `audio` or `text` input to process.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m audio \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 93\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor(audio, \u001b[38;5;241m*\u001b[39margs, sampling_rate\u001b[38;5;241m=\u001b[39msampling_rate, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DataScientest\\lib\\site-packages\\transformers\\models\\wav2vec2\\feature_extraction_wav2vec2.py:196\u001b[0m, in \u001b[0;36mWav2Vec2FeatureExtractor.__call__\u001b[1;34m(self, raw_speech, padding, max_length, truncation, pad_to_multiple_of, return_attention_mask, return_tensors, sampling_rate, **kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m# convert into correct format for padding\u001b[39;00m\n\u001b[0;32m    194\u001b[0m encoded_inputs \u001b[38;5;241m=\u001b[39m BatchFeature({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_values\u001b[39m\u001b[38;5;124m\"\u001b[39m: raw_speech})\n\u001b[1;32m--> 196\u001b[0m padded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoded_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;66;03m# convert input values to correct format\u001b[39;00m\n\u001b[0;32m    206\u001b[0m input_values \u001b[38;5;241m=\u001b[39m padded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_values\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DataScientest\\lib\\site-packages\\transformers\\feature_extraction_sequence_utils.py:203\u001b[0m, in \u001b[0;36mSequenceFeatureExtractor.pad\u001b[1;34m(self, processed_features, padding, max_length, truncation, pad_to_multiple_of, return_attention_mask, return_tensors)\u001b[0m\n\u001b[0;32m    199\u001b[0m     truncated_inputs\u001b[38;5;241m.\u001b[39mappend(inputs_slice)\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padding_strategy \u001b[38;5;241m==\u001b[39m PaddingStrategy\u001b[38;5;241m.\u001b[39mLONGEST:\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;66;03m# make sure that `max_length` cannot be longer than the longest truncated length\u001b[39;00m\n\u001b[1;32m--> 203\u001b[0m     max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_slice\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_input_names\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minput_slice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtruncated_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m     padding_strategy \u001b[38;5;241m=\u001b[39m PaddingStrategy\u001b[38;5;241m.\u001b[39mMAX_LENGTH\n\u001b[0;32m    206\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DataScientest\\lib\\site-packages\\transformers\\feature_extraction_sequence_utils.py:203\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    199\u001b[0m     truncated_inputs\u001b[38;5;241m.\u001b[39mappend(inputs_slice)\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padding_strategy \u001b[38;5;241m==\u001b[39m PaddingStrategy\u001b[38;5;241m.\u001b[39mLONGEST:\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;66;03m# make sure that `max_length` cannot be longer than the longest truncated length\u001b[39;00m\n\u001b[1;32m--> 203\u001b[0m     max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_slice\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_input_names\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m input_slice \u001b[38;5;129;01min\u001b[39;00m truncated_inputs)\n\u001b[0;32m    204\u001b[0m     padding_strategy \u001b[38;5;241m=\u001b[39m PaddingStrategy\u001b[38;5;241m.\u001b[39mMAX_LENGTH\n\u001b[0;32m    206\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mTypeError\u001b[0m: len() of unsized object"
     ]
    }
   ],
   "source": [
    " from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    " from datasets import load_dataset\n",
    " import torch\n",
    " !pip install soundfile librosa\n",
    "\n",
    " # load model and tokenizer\n",
    " processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    " model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "     \n",
    " # load dummy dataset and read soundfiles\n",
    " ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    " \n",
    " # tokenize\n",
    " input_values = processor(np.array(ds.features['audio']), return_tensors=\"pt\", padding=\"longest\").input_values  # Batch size 1\n",
    " \n",
    " # retrieve logits\n",
    " logits = model(input_values).logits\n",
    " \n",
    " # take argmax and decode\n",
    " predicted_ids = torch.argmax(logits, dim=-1)\n",
    " transcription = processor.batch_decode(predicted_ids)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
