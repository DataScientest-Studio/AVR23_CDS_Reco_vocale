{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a996d864-6a89-4513-95ba-2edb457d25be",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Identification de langue avec la tokenisation BERT ou Tiktoken**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cb14523-04a8-424c-976a-d61585c0bbf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Choix de la Tokenisation (False = BERT, True Tiktoken)\n",
    "titoken_tokenization = False\n",
    "\n",
    "## Pour résoudre les problème de mémoire et de performances\n",
    "nb_token_max = 2000\n",
    "nb_phrase_lang = 56000\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bad4ba-8678-41a4-9008-ab1915eddae6",
   "metadata": {},
   "source": [
    "#### Lectures des phrases et de leur étiquette \"Langue\" pour les langues sélectionnées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "491d66b2-90bd-49e9-99cb-9601edf52a57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes de sentence.csv: 280000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lan_code</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spa</td>\n",
       "      <td>Figaro nos invitó a su boda, pero cuando llega...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ita</td>\n",
       "      <td>Ho bisogno di parlarti stanotte.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>deu</td>\n",
       "      <td>John arbeitet im Bereich Neuromarketing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spa</td>\n",
       "      <td>Tú irás a la escuela.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deu</td>\n",
       "      <td>Warum denkt jeder, ich sei dumm?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279995</th>\n",
       "      <td>deu</td>\n",
       "      <td>Ich weiß absolut gar nichts darüber.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279996</th>\n",
       "      <td>ita</td>\n",
       "      <td>Marie ha capito.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279997</th>\n",
       "      <td>deu</td>\n",
       "      <td>Um Missverständnisse zu vermeiden, hat er den ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279998</th>\n",
       "      <td>deu</td>\n",
       "      <td>Tom hatte seit Jahren kein Schwimmbad mehr von...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279999</th>\n",
       "      <td>deu</td>\n",
       "      <td>Ich bin zu klein.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>280000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       lan_code                                           sentence\n",
       "0           spa  Figaro nos invitó a su boda, pero cuando llega...\n",
       "1           ita                   Ho bisogno di parlarti stanotte.\n",
       "2           deu           John arbeitet im Bereich Neuromarketing.\n",
       "3           spa                              Tú irás a la escuela.\n",
       "4           deu                   Warum denkt jeder, ich sei dumm?\n",
       "...         ...                                                ...\n",
       "279995      deu               Ich weiß absolut gar nichts darüber.\n",
       "279996      ita                                   Marie ha capito.\n",
       "279997      deu  Um Missverständnisse zu vermeiden, hat er den ...\n",
       "279998      deu  Tom hatte seit Jahren kein Schwimmbad mehr von...\n",
       "279999      deu                                  Ich bin zu klein.\n",
       "\n",
       "[280000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ouvrir le fichier d'entrée en mode lecture\n",
    "def create_lang_df(path):\n",
    "    df = pd.read_csv(path, index_col ='id')\n",
    "    return df\n",
    "\n",
    "df_big = create_lang_df('../data/multilingue/sentences.csv')\n",
    "lan_code = ['eng','fra','deu','spa','ita']\n",
    "df = pd.DataFrame(columns=df_big.columns)\n",
    "for i in range(len(lan_code)):\n",
    "    df= pd.concat([df, df_big[df_big['lan_code']==lan_code[i]].iloc[:nb_phrase_lang]])\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "n_rows = len(df)\n",
    "print('Nombre de lignes de sentence.csv:',n_rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71dc774-4916-4658-b015-01594a8d003b",
   "metadata": {},
   "source": [
    "#### Réalisation d'un jeu de données d'entrainement et de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dbac26f-2f26-4e3e-9685-1aa5914569b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lan_code</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>deu</td>\n",
       "      <td>Sie sagte: „Auf Wiedersehen!“</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spa</td>\n",
       "      <td>Me lo he jugado el todo por el todo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spa</td>\n",
       "      <td>Me pregunto si Tom y Mary están locos.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eng</td>\n",
       "      <td>I've lived here my whole life.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ita</td>\n",
       "      <td>Pensate che Mary sia troppo grassa per essere una cheerleader?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139995</th>\n",
       "      <td>deu</td>\n",
       "      <td>„Das darf ich nicht verraten!“ – „Was würde passieren, wenn du’s doch tätest?“</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139996</th>\n",
       "      <td>ita</td>\n",
       "      <td>Ken è andato al supermercato a comprare delle uova.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139997</th>\n",
       "      <td>fra</td>\n",
       "      <td>Si ton frère te dit : je suis pauvre, et j'ai froid, / Ton devoir est d'offrir la moitié de ton toit / À ton frère.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139998</th>\n",
       "      <td>deu</td>\n",
       "      <td>Wir beabsichtigen mit allen Ländern, die uns umgeben, friedlich zusammenzuarbeiten.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139999</th>\n",
       "      <td>fra</td>\n",
       "      <td>Un de mes voisins a appelé et dit que j'avais laissé une de mes fenêtres ouvertes.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       lan_code  \\\n",
       "0           deu   \n",
       "1           spa   \n",
       "2           spa   \n",
       "3           eng   \n",
       "4           ita   \n",
       "...         ...   \n",
       "139995      deu   \n",
       "139996      ita   \n",
       "139997      fra   \n",
       "139998      deu   \n",
       "139999      fra   \n",
       "\n",
       "                                                                                                                   sentence  \n",
       "0                                                                                             Sie sagte: „Auf Wiedersehen!“  \n",
       "1                                                                                      Me lo he jugado el todo por el todo.  \n",
       "2                                                                                    Me pregunto si Tom y Mary están locos.  \n",
       "3                                                                                            I've lived here my whole life.  \n",
       "4                                                            Pensate che Mary sia troppo grassa per essere una cheerleader?  \n",
       "...                                                                                                                     ...  \n",
       "139995                                       „Das darf ich nicht verraten!“ – „Was würde passieren, wenn du’s doch tätest?“  \n",
       "139996                                                                  Ken è andato al supermercato a comprare delle uova.  \n",
       "139997  Si ton frère te dit : je suis pauvre, et j'ai froid, / Ton devoir est d'offrir la moitié de ton toit / À ton frère.  \n",
       "139998                                  Wir beabsichtigen mit allen Ländern, die uns umgeben, friedlich zusammenzuarbeiten.  \n",
       "139999                                   Un de mes voisins a appelé et dit que j'avais laissé une de mes fenêtres ouvertes.  \n",
       "\n",
       "[140000 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[27860, 27929, 28006, 27986, 28219]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# créer 2 dataframes: 1 train (50% des phrases) et 1 test (50% des phrases)\n",
    "n_train = int(n_rows*0.5)\n",
    "df_train = df.iloc[:n_train].sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_test = df.iloc[n_train:].sample(frac=1, random_state=24).reset_index(drop=True)\n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "display(df_train)\n",
    "nb_phrases_lang =[]\n",
    "for l in lan_code:\n",
    "    nb_phrases_lang.append(sum(df_train['lan_code']==l))\n",
    "nb_phrases_lang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78bb348-f3c0-4455-b39e-a0b26527556a",
   "metadata": {},
   "source": [
    "#### Selection du tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce260244-a177-43b5-accf-0564548c2fe5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Selection du tokenizer\n",
    "if titoken_tokenization:\n",
    "    import tiktoken\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "else:\n",
    "    from transformers import BertTokenizerFast\n",
    "    tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cb9b00-6cc3-47bc-afcf-b00fdef8738b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Tokenisation des données sélectionnées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df235a48-3ea5-47a7-af0a-f91fd20af84e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de tokens avant plafonnement: 31877\n",
      "Nombre de tokens après plafonnement: 2000\n",
      "Liste des 50 tokens les plus fréquents: ['[CLS]', '[SEP]', '.', ',', \"'\", 'a', '?', 'de', 'la', 'tom', '##s', 'que', 'the', 'i', 'un', 'in', '##e', 'il', 'es', 'to', 'ich', 'en', 'le', 'el', 'e', '!', 'je', '-', 'no', 'est', 'non', '##o', 'me', 'die', 'ist', 'you', 'l', 'di', 'is', 'pas', 'tu', 'nicht', 'sie', 'der', 'se', 'du', '##n', '##r', 'con', 't']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "def save_dict_token(dict_ids):\n",
    "    with open('../data/dict_token', 'wb') as fichier:\n",
    "        pickle.dump(dict_ids, fichier)\n",
    "\n",
    "def load_dict_token():\n",
    "    with open('../data/dict_token', 'rb') as fichier:\n",
    "        dict_ids = pickle.load(fichier)\n",
    "        # Définition d'une liste 'écrite' des tokens\n",
    "        decoded_keys = [tokenizer.decode([key]) for key in list(dict_ids.keys())]\n",
    "    return dict_ids, decoded_keys\n",
    "\n",
    "# Création d'un dictionnaire de complet des Token ID à partir des phrases selectionnée\n",
    "dict_ids = Counter(token for ligne in df['sentence'] for token in tokenizer.encode(ligne))\n",
    "\n",
    "# Tri des token en fonction de leur fréquence\n",
    "dict_ids= sorted(dict_ids.items(), key=lambda x: x[1], reverse=True) \n",
    "print(\"Nombre de tokens avant plafonnement:\",len(dict_ids))\n",
    "print(\"Nombre de tokens après plafonnement:\",min(len(dict_ids),nb_token_max))\n",
    "\n",
    "# Limitation du nombre de token\n",
    "dict_ids = dict(dict_ids[:nb_token_max])\n",
    "# save_dict_token(dict_ids)\n",
    "\n",
    "# Définition d'une liste 'écrite' des tokens\n",
    "decoded_keys = [tokenizer.decode([key]) for key in list(dict_ids.keys())]\n",
    "print(\"Liste des 50 tokens les plus fréquents:\",decoded_keys[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adba3da-befd-44cd-a602-13b87cb664e6",
   "metadata": {},
   "source": [
    "#### Création d'un Bag Of Worlds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "582329fa-fe9a-4e42-bc24-5349be9f0445",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Créez un DataFrame BOW avec les phrases (lignes) et les fréquences de chaque token (colonnes)\n",
    "def create_BOW(data):\n",
    "    BOW = []\n",
    "    for ligne in data:\n",
    "        l_tokenised = tokenizer.encode(ligne)\n",
    "        BOW.append([l_tokenised.count(token) for token in dict_ids])\n",
    "    return BOW\n",
    "            \n",
    "X_train = create_BOW(df_train['sentence'])\n",
    "y_train = df_train['lan_code'].values.tolist() \n",
    "\n",
    "#\n",
    "X_test = create_BOW(df_test['sentence'])\n",
    "y_test = df_test['lan_code'].values.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a96239-98ab-47b0-acf9-1660e16fc9ab",
   "metadata": {},
   "source": [
    "#### Choix du nom du fichier de sauvegarde du classifieur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acc778b1-231d-42c6-9218-144ee7e88da1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_file_name(titoken_tokenization, classifier):\n",
    "    if titoken_tokenization:\n",
    "        return \"id_lang_tiktoken_\"+classifier+\".pkl\"\n",
    "    else:\n",
    "        return \"id_lang_BERT_\"+classifier+\".pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923233dd-f03a-400a-bc7c-b5fc4914ca8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Création d'un classificateur avec l'algorithme Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c40a11f5-cf0d-4c5f-8ee1-29f6a05bf4df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/id_lang_BERT_nb.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import naive_bayes\n",
    "\n",
    "# On entraine et on prédit:\n",
    "clf_nb = naive_bayes.MultinomialNB()  # BernoulliNB() # MultinomialNB() \n",
    "clf_nb.fit(X_train, y_train)\n",
    "\n",
    "# Save the model to a file\n",
    "# joblib.dump(clf_nb, \"../data/\"+get_file_name(titoken_tokenization,\"nb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54b3449f-f635-402a-b3bf-5c163fadad52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Classe prédite</th>\n",
       "      <th>deu</th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>ita</th>\n",
       "      <th>spa</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Classe réelle</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>deu</th>\n",
       "      <td>27911</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eng</th>\n",
       "      <td>25</td>\n",
       "      <td>27991</td>\n",
       "      <td>17</td>\n",
       "      <td>58</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fra</th>\n",
       "      <td>15</td>\n",
       "      <td>26</td>\n",
       "      <td>27717</td>\n",
       "      <td>139</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ita</th>\n",
       "      <td>23</td>\n",
       "      <td>48</td>\n",
       "      <td>98</td>\n",
       "      <td>27373</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spa</th>\n",
       "      <td>22</td>\n",
       "      <td>33</td>\n",
       "      <td>68</td>\n",
       "      <td>292</td>\n",
       "      <td>27599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Classe prédite    deu    eng    fra    ita    spa\n",
       "Classe réelle                                    \n",
       "deu             27911     26     12     25     20\n",
       "eng                25  27991     17     58     49\n",
       "fra                15     26  27717    139    174\n",
       "ita                23     48     98  27373    239\n",
       "spa                22     33     68    292  27599"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Naïve Bayes = 0.990\n"
     ]
    }
   ],
   "source": [
    "# Load the model from a file\n",
    "# clf_nb = joblib.load(\"../data/\"+get_file_name(titoken_tokenization,\"nb\"))\n",
    "# dict_ids, decoded_keys = load_dict_token()\n",
    "\n",
    "y_pred_nb = clf_nb.predict(X_test)\n",
    "accuracy_naive_bayes = accuracy_score(y_test, y_pred_nb)\n",
    "display(pd.crosstab(y_test,y_pred_nb,rownames=['Classe réelle'], colnames=['Classe prédite']))\n",
    "print(\"Accuracy Naïve Bayes = {:.3f}\".format(accuracy_naive_bayes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95249cde-cfc5-487c-b579-c590ee84ca42",
   "metadata": {},
   "source": [
    "#### Création d'un classificateur avec l'algorithme Gradiant Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffa3cdc-84f7-4d08-acc9-c3f975be2e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Créer un classificateur clf et entraîner le modèle sur l'ensemble d'entraînement\n",
    "clf_gb = GradientBoostingClassifier(n_estimators=100, learning_rate=1, max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "\n",
    "# Save the model to a file\n",
    "joblib.dump(clf_gb, \"../data/\"+get_file_name(titoken_tokenization,\"gb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfa6b8c-172e-4cb7-ab10-20ed7154c245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from a file\n",
    "clf_gb = joblib.load(\"../data/\"+get_file_name(titoken_tokenization,\"gb\"))\n",
    "dict_ids, decoded_keys = load_dict_token()\n",
    "\n",
    "# Calculer les prédictions \n",
    "y_pred_gb = clf_gb.predict(X_test)\n",
    "accuracy_gradiant_boosting = accuracy_score(y_test, y_pred_gb)\n",
    "display(pd.crosstab(y_test,y_pred_gb,rownames=['Classe réelle'], colnames=['Classe prédite']))\n",
    "print(\"Accuracy Gradiant Boosting = {:.3f}\".format(accuracy_gradiant_boosting))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df82b0a7-3649-4c66-97d2-e38c52e2fbfc",
   "metadata": {},
   "source": [
    "#### Exemples d'utilisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272d7135-8ec5-4294-bfe4-4536184e8518",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instanciation d'un exemple\n",
    "exemple = [\"france is often snowy during spring , and it is relaxing in january .\",\n",
    "           \"elle adore les voitures très luxueuses, et toi ?\",\n",
    "           \"she loves very luxurious cars, don't you?\",\n",
    "           \"vamos a la playa\",\n",
    "           \"Ich heiße Keyne, und das ist wunderbar\",\n",
    "           \"she loves you, mais elle te hait aussi, and das ist traurig\", # Attention à cette phrase trilingue\n",
    "           \"I ate caviar.\", \n",
    "           \"Vogliamo visitare il Colosseo e nuotare nel Tevere.\"\n",
    "          ]\n",
    "\n",
    "\n",
    "# Prédiction des classes de l'exemple par le classifieur Naive Bayes\n",
    "predictions_nb = clf_nb.predict(create_BOW(exemple))\n",
    "\n",
    "# Prédiction des classes de l'exemple par le classifieur Gradiant Boosting\n",
    "predictions_gb = clf_gb.predict(create_BOW(exemple))\n",
    "\n",
    "# Affichage des prédictions\n",
    "print('Langue réelle                 :',['eng','fra','eng','spa','deu','en,fr,de','en','ita'])\n",
    "print('Prédictions Naive Bayes       :',predictions_nb)\n",
    "print('Prédictions Gradiant Boosting :',predictions_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d1c95d-fc35-4d66-9ad4-c014371cd8d9",
   "metadata": {},
   "source": [
    "#### **Analyse en Composante Principale pour visualiser l'importance des tokens**\n",
    "#### dans la détection de langue par le classifieur Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dc7938-4010-4f98-8cae-ba0da53c6372",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "\n",
    "# max_lines = 35000\n",
    "# max_cols = 20000\n",
    "# [ligne[:max_cols] for ligne in X_train[:max_lines]]\n",
    "pd_X_train = pd.DataFrame(data=X_train, columns = decoded_keys)\n",
    "n = pd_X_train.shape[1]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(pd_X_train)\n",
    "\n",
    "X_new = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "coeff = pca.components_.transpose()\n",
    "\n",
    "xs = X_new[:, 0]\n",
    "ys = X_new[:, 1]\n",
    "scalex = 1.0/(xs.max() - xs.min())\n",
    "scaley = 1.0/(ys.max() - ys.min())\n",
    "\n",
    "\n",
    "principalDf = pd.DataFrame({'PC1': xs*scalex, 'PC2': ys * scaley})\n",
    "\n",
    "y_train_pred = y_pred_nb #clf_nb.predict(X_train_scaled)\n",
    "finalDF = pd.concat([principalDf, pd.Series(\n",
    "    y_train_pred, name='Langue')], axis=1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='Langue', data=finalDF, alpha=0.5)\n",
    "\n",
    "for i in range(min(n,50)):\n",
    "    plt.arrow(0, 0, coeff[i, 0]*1.5, coeff[i, 1]*1.5,\n",
    "              color='k', alpha=0.08, head_width=0.002, )\n",
    "    plt.text(coeff[i, 0]*1.5, coeff[i, 1] * 1.5, pd_X_train.columns[i], color='k')\n",
    "\n",
    "plt.title(\"Importance des principaux tokens dans l'identification de langue par l'algorithme Naive Bayes\") \n",
    "plt.xlim(-0.2, 0.2);\n",
    "plt.ylim(-0.2, 0.2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19701854-6f09-4fcb-bed0-130a936252a4",
   "metadata": {},
   "source": [
    "### **Algorithme LIME appliqué à une phrase mal classée par Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33938475-6652-481b-9ff0-b41237673f37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from skater.core.local_interpretation.lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "# Instanciation d'un exemple\n",
    "exemple = [\"Venite dentro. Insisto\",\n",
    "           \"I visited Hokkaido during summer vacation.\",\n",
    "           \"she loves very luxurious cars, don't you?\",\n",
    "           \"vamos a la playa\",\n",
    "           \"Ich heiße Keyne, und das ist wunderbar\",\n",
    "           \"she loves you, but she te hait aussi, and das ist traurig\", # Attention à cette phrase trilingue\n",
    "           \"I ate caviar.\",\n",
    "           \"Vogliamo visitare il Colosseo e nuotare nel Tevere.\",\n",
    "          ]\n",
    "\n",
    "doc_num = 0\n",
    "# Nous reconstruisons le même modèle mais avec des numpy arrays pour éviter les problème avec LIME du au nomns des features\n",
    "features = np.array(create_BOW([exemple[doc_num]]))\n",
    "#print(features)\n",
    "exp = LimeTabularExplainer(features, feature_names=list(decoded_keys), \n",
    "                           class_names=['deu', 'eng','fra','ita','spa'])\n",
    "print(\"Phrase :\",exemple[doc_num])\n",
    "print('Predicted Label:',clf_nb.predict(features)[0],' ',clf_nb.predict_proba(features)[0])\n",
    "exp.explain_instance(features[0], clf_nb.predict_proba, top_labels=6).show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e0ead5-2836-4c04-b7fb-9b41074d345a",
   "metadata": {},
   "source": [
    "### **Interprétation du processus d'identification d'une langue avec Naïve Bayes**\n",
    "#### 1- Création d'un BOW Train avec les Token ID en colonne + le lan_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2619611c-6b53-4b26-9f21-62b766226df3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "col_name = [str(key) for key in list(dict_ids.keys())]\n",
    "df_BOW = pd.DataFrame(data=X_train, columns=col_name)\n",
    "df_BOW['lan_code']=y_train\n",
    "display(df_BOW.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75799ec2-58e7-4693-97bf-6ec051abfb95",
   "metadata": {},
   "source": [
    "#### 2- Calcul du nombre d'apparitions des tokens dans chaque langue\n",
    "#### 3- Calcul de la probabilité d'apparition de chaque token dans chaque langue \n",
    "#### 4- Calcul (par multiplication) de la probabilité d'appartenance de la phrase à une langue\n",
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/7e2c830eac90468e5839385df574ae31d4fc1dbc\" style=\"height:50px\">\n",
    "\n",
    "> où **C** est la classe (*lan_code*), **Fi** est la *caractéristique i* du BOW, **Z** est l'*\"evidence\"* servant à regulariser la proba "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96157c37-4556-45e0-9cc3-6212180dd263",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instanciation d'un exemple\n",
    "exemple = [\"Passo a controllare alle otto.\",\n",
    "           \"Voilà la mariée !\",\n",
    "           \"she loves very luxurious cars, don't you?\",\n",
    "           \"vamos a la playa\",\n",
    "           \"Ich heiße Keyne, und das ist wunderbar\",\n",
    "           \"she loves you, but she te hait aussi, and das ist traurig\", # Attention à cette phrase trilingue\n",
    "           \"I ate fruits.\",\n",
    "           \"Venite dentro. Insisto\",\n",
    "           \"Tom smelled funny. \",\n",
    "          ]\n",
    "lang_exemple = ['ita','fra','eng','spa','deu','eng,fra,deu', 'eng','ita','eng']\n",
    "                \n",
    "# Selection de la phrase à analyser\n",
    "sel_phrase = 8\n",
    "print(\"Phrase à analyser :\\033[31;46m\",lang_exemple[sel_phrase],'- \"'+exemple[sel_phrase]+'\\033[0m')\n",
    "\n",
    "# Tokenisation et encodage de la phrase\n",
    "encodage = tokenizer.encode(exemple[sel_phrase])\n",
    "print(\"Nombre de tokens dans la phrase:\",len(encodage))\n",
    "\n",
    "# Création du vecteur BOW de la phrase\n",
    "bow_exemple = create_BOW([exemple[sel_phrase]])\n",
    "print(\"Nombre de tokens retenus dans le BOW:\",sum(bow_exemple[0]))\n",
    "masque_tokens_retenus = [(1 if token in dict_ids else 0) for token in encodage]\n",
    "print(\"\\033[96;43mTokens retenus\\033[0m (se trouvant dans le modèle)     :\",end=\"\")\n",
    "for i in range(len(encodage)):\n",
    "    if masque_tokens_retenus[i]==1:\n",
    "        print(\"\\033[96;43m\"+tokenizer.decode([encodage[i]])+\"\\033[0m \",end=\"\")\n",
    "    else: print(\"\\033[31;47m\"+tokenizer.decode([encodage[i]])+\"\\033[0m \",end=\"\")\n",
    "\n",
    "print(\"\")\n",
    "# Afin de continuer l'analyse on ne garde que les token de la phrase disponibles dans le BOW\n",
    "token_used = [str(encodage[i]) for i in range(len(encodage)) if (masque_tokens_retenus[i]==1)]\n",
    "\n",
    "# Calcul du nombre d'apparition de ces tokens dans le BOW pour chaque langue, et stockage dans un DataFrame df_count\n",
    "def compter_non_zero(colonne):\n",
    "    return (colonne != 0).sum()\n",
    "\n",
    "votes = []\n",
    "for i in range(len(lan_code)):\n",
    "    #votes.append(list(df_BOW[token_used].loc[df_BOW['lan_code']==lan_code[i]].sum(axis=0)))\n",
    "    votes.append(list(df_BOW[token_used].loc[df_BOW['lan_code']==lan_code[i]].apply(compter_non_zero)))\n",
    "\n",
    "col_name = [str(i+1)+'-'+tokenizer.decode([int(token_used[i])]) for i in range(len(token_used))]\n",
    "df_count = pd.DataFrame(data=votes,columns=token_used, index=lan_code)\n",
    "df_count.columns = col_name\n",
    "display(df_count)\n",
    "\n",
    "# Calcul de la probabilité d'apparition de chaque token dans chaque langue\n",
    "df_proba = df_count.div(nb_phrases_lang, axis = 0)\n",
    "\n",
    "# Calcul (par multiplication) de la probabilité d'appartenance de la phrase à une langue\n",
    "df_proba['Proba'] = 1\n",
    "# Itérer sur les colonnes et effectuez la multiplication pour chaque ligne\n",
    "df_proba = df_proba.replace(0.0,0.000001)\n",
    "for col in df_count.columns:\n",
    "    df_proba['Proba'] *= df_proba[col]\n",
    "# Multiplier par la probabilité de la classe\n",
    "p_classe = [(nb_phrases_lang[i]/df_BOW.shape[0]) for i in range(len(nb_phrases_lang))]\n",
    "df_proba['Proba'] *= p_classe\n",
    "# Diviser par l'evidence\n",
    "evidence = df_proba['Proba'].sum(axis=0)\n",
    "df_proba['Proba'] *= 1/evidence\n",
    "df_proba['Proba'] = df_proba['Proba'].round(3)\n",
    "\n",
    "\n",
    "# Affichage de la matrice des probabilités\n",
    "display(df_proba)\n",
    "\n",
    "print(\"Langue réelle de la phrase                         : \",lang_exemple[sel_phrase])\n",
    "print(\"Langue dont la probabilité est la plus forte       : \",df_proba['Proba'].idxmax())\n",
    "prediction = clf_nb.predict(bow_exemple) \n",
    "print(\"Langue dont la probabilité prédite par Naiva Bayes : \",prediction[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed8d1c1-e5dd-4a17-8aee-1305fbf4c45a",
   "metadata": {},
   "source": [
    "> Recherche des phrases mal classées par Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d458174a-c2e1-41e8-a84a-35584eaac751",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_bad_max = 30\n",
    "n_bad = 0\n",
    "for i in range(len(y_test)):\n",
    "    if (y_test[i] != y_pred_nb[i]) and (n_bad<n_bad_max):\n",
    "        n_bad +=1\n",
    "        print(n_bad,'- No',i,' - Réel:',y_test[i],' Prédit:',y_pred_nb[i],'    ',df_test['sentence'].iloc[i],\" (proba={:.2f}\".format(max(clf_nb.predict_proba(np.array(create_BOW([df_test['sentence'].iloc[i]])))[0]))+\")\")\n",
    "\n",
    "# s = \"Tom smelled funny. \"\n",
    "# print(s,\" -> \", clf_nb.predict(create_BOW([s]))[0],\" (proba={:.2f}\".format(max(clf_nb.predict_proba(create_BOW([s]))[0]))+\")\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
