{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a996d864-6a89-4513-95ba-2edb457d25be",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Identification de 95 langues avec :**\n",
    ">### **- des '*Sparse*' Bag Of Words**\n",
    ">### **- une Tokenisations Tiktoken**\n",
    ">### **- CountVectorizer utilisant une tokenisation '*custom*'**\n",
    ">### **- un Classificateurs Naïve Bayes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e67500-a4c1-4d58-8f4a-1be20434be6d",
   "metadata": {},
   "source": [
    "## **1 - Contruction des classificateurs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8ef517-3030-4c4c-904f-0db3d49140df",
   "metadata": {},
   "source": [
    "#### **Chargement des biblothèques nécéssaires** <font color='red'>(nécéssaire pour traduction texte libre)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cb14523-04a8-424c-976a-d61585c0bbf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tiktoken\n",
    "import random\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Ce parametre permet éventuellement d'équilibrer de nombre de phrase par langue.\n",
    "# Si ce parametre est très grand, tout le corpus sera lu. \n",
    "nb_phrase_lang = 10000000\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bad4ba-8678-41a4-9008-ab1915eddae6",
   "metadata": {},
   "source": [
    "#### **Lectures des phrases de \"sentences-big.csv\", et de leur étiquette \"Langue\" pour les langues sélectionnées**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "491d66b2-90bd-49e9-99cb-9601edf52a57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes de sentence.csv: 10341812\n",
      "Nombre de langues à classer: 404\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lan_code</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ita</td>\n",
       "      <td>Il tuo futuro è pieno di possibilità.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fra</td>\n",
       "      <td>J'aimerais aller en France, un jour.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>epo</td>\n",
       "      <td>La polica enketo aperigis ilian sekretan vivon.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kab</td>\n",
       "      <td>Kullec ifukk yid-k.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hun</td>\n",
       "      <td>Több munkát nem tudok elvállalni.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10341807</th>\n",
       "      <td>deu</td>\n",
       "      <td>Wir werden das Problem nicht aufgreifen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10341808</th>\n",
       "      <td>fra</td>\n",
       "      <td>Je suis cuit !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10341809</th>\n",
       "      <td>kab</td>\n",
       "      <td>Isefk fell-ak a tregleḍ iɣis-a.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10341810</th>\n",
       "      <td>tok</td>\n",
       "      <td>o pana ala e moku tawa soweli tomo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10341811</th>\n",
       "      <td>hun</td>\n",
       "      <td>Máris unod magad?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10341812 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         lan_code                                         sentence\n",
       "0             ita            Il tuo futuro è pieno di possibilità.\n",
       "1             fra             J'aimerais aller en France, un jour.\n",
       "2             epo  La polica enketo aperigis ilian sekretan vivon.\n",
       "3             kab                              Kullec ifukk yid-k.\n",
       "4             hun                Több munkát nem tudok elvállalni.\n",
       "...           ...                                              ...\n",
       "10341807      deu         Wir werden das Problem nicht aufgreifen.\n",
       "10341808      fra                                   Je suis cuit !\n",
       "10341809      kab                  Isefk fell-ak a tregleḍ iɣis-a.\n",
       "10341810      tok              o pana ala e moku tawa soweli tomo.\n",
       "10341811      hun                                Máris unod magad?\n",
       "\n",
       "[10341812 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ouvrir le fichier d'entrée en mode lecture\n",
    "def create_lang_df(path):\n",
    "    df = pd.read_csv(path, index_col ='id')\n",
    "    return df\n",
    "\n",
    "df = create_lang_df('../data/multilingue/sentences-big.csv')\n",
    "lan_code = list(set(df['lan_code']))\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "n_rows = len(df)\n",
    "print('Nombre de lignes de sentence.csv:',n_rows)\n",
    "print('Nombre de langues à classer:',len(lan_code))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71dc774-4916-4658-b015-01594a8d003b",
   "metadata": {},
   "source": [
    "#### **Réalisation d'un jeu de données d'entrainement et de test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dbac26f-2f26-4e3e-9685-1aa5914569b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lan_code</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cmn</td>\n",
       "      <td>咱们停止争吵和好吧。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fra</td>\n",
       "      <td>Sami a acheté une bouteille de vin.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fra</td>\n",
       "      <td>Tom a du mal à comprendre ce concept.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rus</td>\n",
       "      <td>Они идут друг за другом.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spa</td>\n",
       "      <td>Quisiera enviar este paquete a Japón.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9824715</th>\n",
       "      <td>rus</td>\n",
       "      <td>Давайте смотреть фактам в лицо!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9824717</th>\n",
       "      <td>jpn</td>\n",
       "      <td>私はそれについて全く知りません。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9824718</th>\n",
       "      <td>rus</td>\n",
       "      <td>Как я сегодня счастлив!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9824719</th>\n",
       "      <td>epo</td>\n",
       "      <td>Mi esperas, ke baldaŭ ni povu kunlabori.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9824720</th>\n",
       "      <td>rus</td>\n",
       "      <td>В последний раз я видел их в Албании.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9753920 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        lan_code                                  sentence\n",
       "0            cmn                                咱们停止争吵和好吧。\n",
       "1            fra       Sami a acheté une bouteille de vin.\n",
       "2            fra     Tom a du mal à comprendre ce concept.\n",
       "3            rus                  Они идут друг за другом.\n",
       "4            spa     Quisiera enviar este paquete a Japón.\n",
       "...          ...                                       ...\n",
       "9824715      rus           Давайте смотреть фактам в лицо!\n",
       "9824717      jpn                          私はそれについて全く知りません。\n",
       "9824718      rus                   Как я сегодня счастлив!\n",
       "9824719      epo  Mi esperas, ke baldaŭ ni povu kunlabori.\n",
       "9824720      rus     В последний раз я видел их в Албании.\n",
       "\n",
       "[9753920 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes par langue:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nb_phrases_lang</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lan_code</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>afr</th>\n",
       "      <td>4137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ara</th>\n",
       "      <td>38650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arq</th>\n",
       "      <td>2326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asm</th>\n",
       "      <td>3205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avk</th>\n",
       "      <td>4102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>war</th>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wuu</th>\n",
       "      <td>4757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yid</th>\n",
       "      <td>9603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yue</th>\n",
       "      <td>6230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zsm</th>\n",
       "      <td>6610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          nb_phrases_lang\n",
       "lan_code                 \n",
       "afr                  4137\n",
       "ara                 38650\n",
       "arq                  2326\n",
       "asm                  3205\n",
       "avk                  4102\n",
       "...                   ...\n",
       "war                  2025\n",
       "wuu                  4757\n",
       "yid                  9603\n",
       "yue                  6230\n",
       "zsm                  6610\n",
       "\n",
       "[95 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# créer 2 dataframes: 1 train (95% des phrases) et 1 test (5% des phrases)\n",
    "n_train = int(n_rows*0.95)\n",
    "df_train = df.iloc[:n_train].sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_test = df.iloc[n_train:].sample(frac=1, random_state=24).reset_index(drop=True)\n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "df_lan = pd.DataFrame(data= df.groupby('lan_code').size(), columns = ['nb_phrases_lang'] )\n",
    "\n",
    "# Filtrage des langues qui ont peu de phrases (>2000)\n",
    "df_lan = df_lan.loc[df_lan['nb_phrases_lang']>=2000]\n",
    "list_lan = list(set(df_lan.index))\n",
    "df_train = df_train[df_train['lan_code'].isin(list_lan)]\n",
    "df_test = df_test[df_test['lan_code'].isin(list_lan)]\n",
    "print('df_train:')\n",
    "display(df_train)\n",
    "print('Nombre de lignes par langue:')\n",
    "display(df_lan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1ccdf5-a5c4-4181-8b5a-22a3148d331a",
   "metadata": {},
   "source": [
    "#### **Préparation de la vectorisation par CountVectorizer** <font color='red'>(nécéssaire pour traduction texte libre)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9be0a0e5-ac97-4d8a-93fb-bc4785e90808",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Selection du tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Les 2 fonctions suivantes sont nécéssaires afin de sérialiser ces parametre de CountVectorizer\n",
    "# et ainsi de sauvegarder le vectorizer pour un un usage ultérieur sans utiliser X_train pour  le réinitialiser\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    tokens = tokenizer.encode(text)  # Cela divise le texte en mots\n",
    "    return tokens\n",
    "\n",
    "def custom_preprocessor(text):\n",
    "    return text\n",
    "\n",
    "# CountVectorizer a une liste de phrase en entrée.\n",
    "# Cette fonction met les données d'entrée dans le bon format\n",
    "def format_to_vectorize(data):\n",
    "    X_tok = []\n",
    "    if \"DataFrame\" in str(type(data)):sentences = df.tolist()\n",
    "    elif \"str\" in str(type(data)):\n",
    "        sentences =[data]\n",
    "    else: sentences = data\n",
    "                          \n",
    "    for sentence in sentences:\n",
    "        X_tok.append(sentence) # ('¤'.join([tokenizer.decode([ids]) for ids in tokenizer.encode(sentence)])+'¤')\n",
    "    return X_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adba3da-befd-44cd-a602-13b87cb664e6",
   "metadata": {},
   "source": [
    "#### **Création de la fonction Vectorizer et de la fonction de création d'un Bags  Of Worlds** <font color='red'>(nécéssaire pour traduction texte libre)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "029f861d-e36b-47e0-a43d-664e6f00720c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Création d'un vectorizer et du sparse BOW (X_train) avec le nombre d'apparitions\n",
    "global vectorizer, dict_ids, dict_token\n",
    "\n",
    "def create_vectorizer(X_train_tok):\n",
    "    global vectorizer, dict_ids, dict_token\n",
    "    \n",
    "    # token_pattern = r\"[a-zA-Z0-9\\s\\.\\,\\?\\:\\;]+\" \n",
    "    # vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=lambda x: tokenizer.encode(x), preprocessor=lambda x: x) #,token_pattern=token_pattern\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=custom_tokenizer, preprocessor=custom_preprocessor) #,token_pattern=token_pattern\n",
    "    vectorizer.fit(X_train_tok)\n",
    "    \n",
    "    # Création de dictionnaire des Token et des ids \n",
    "    dict_token = {tokenizer.decode([cle]): cle for cle, valeur in vectorizer.vocabulary_.items()}\n",
    "    dict_ids = {cle: tokenizer.decode([cle]) for cle, valeur in vectorizer.vocabulary_.items()} #dict_ids.items()}\n",
    "    return \n",
    "\n",
    "def create_BOW(data, vectorizer_to_create=False):\n",
    "    global vectorizer\n",
    "    \n",
    "    X_tok = format_to_vectorize(data)\n",
    "    if vectorizer_to_create:\n",
    "        create_vectorizer(X_tok)\n",
    "    X = vectorizer.transform(X_tok)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de78ee6-5749-4fe2-bbb4-6ae330e6415a",
   "metadata": {},
   "source": [
    "#### **Création du BOW Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88502575-7222-4b4e-a615-e69b882830a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = create_BOW(df_train['sentence'], True)\n",
    "y_train = df_train['lan_code'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f4988d-49c4-43d1-8219-4971eea7d0ad",
   "metadata": {},
   "source": [
    "#### **Création du BOW Test**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2e470a4-55c9-4516-8136-1331a5984720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test = create_BOW(df_test['sentence'])\n",
    "y_test = df_test['lan_code'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07204db8-0bd2-4742-b518-c12e5d28afe6",
   "metadata": {},
   "source": [
    "#### **Sauvegarde/Chargement du vectorizer** <font color='red'>(nécéssaire pour traduction texte libre)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04628919-c011-407a-8100-ff9bbd4b97e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definition de fonction de sauvegarde et chargement du dictionnaire des tokens utilisés\n",
    "def save_vectorizer(vectorizer):\n",
    "    path = '../data/vectorizer_tiktoken_big.pkl'\n",
    "    joblib.dump(vectorizer, path)\n",
    "\n",
    "def load_vectorizer():\n",
    "    global dict_token, dict_ids, nb_token\n",
    "    \n",
    "    path = '../data/vectorizer_tiktoken_big.pkl'\n",
    "    vectorizer = joblib.load(path)\n",
    "    dict_token = {tokenizer.decode([cle]): cle for cle, valeur in vectorizer.vocabulary_.items()}\n",
    "    dict_ids = {cle: tokenizer.decode([cle]) for cle, valeur in vectorizer.vocabulary_.items()} #dict_ids.items()}\n",
    "    nb_token = len(vectorizer.vocabulary_)\n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c62c934-15d2-4a7e-9300-6cb288d3fcce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save_vectorizer(vectorizer)\n",
    "\n",
    "vectorizer = load_vectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aad8bf-4b26-411e-84be-4280d9940bd5",
   "metadata": {},
   "source": [
    "#### **Definition d'une fonction ids->colonne de X_train et col->ids**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ded52ed4-1600-454a-83f6-196321e9a341",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ids2col(list_ids):\n",
    "    d = dict(vectorizer.vocabulary_.items())\n",
    "    list_col = []\n",
    "    for ids in list_ids:\n",
    "        list_col.append(d[ids])\n",
    "    return list_col\n",
    "\n",
    "def col2ids(list_col):\n",
    "    d = dict(vectorizer.vocabulary_.items())\n",
    "    list_ids = []\n",
    "    for col in list_col:\n",
    "        for ids, c in d.items():\n",
    "            if col==c:\n",
    "                list_ids.append(ids)\n",
    "                break\n",
    "    return list_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a164735-49e1-4944-a92a-2c4e375945c9",
   "metadata": {},
   "source": [
    "#### **Création d'un dictionnaire des tokens avec leur fréquence d'apparition dans Train**\n",
    "#### **Définition d'une liste de token trié par fréquence d'apparition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "508009b3-b248-46b3-9f5a-0c5c93e35e00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de tokens : 59122\n",
      "Liste des 50 tokens les plus fréquents: ['.', ',', '?', ' a', 'i', 'Tom', 'a', 'u', ' to', ' la', ' de', ' ', ' t', ' d', 'I', 'as', ' Tom', ' k', 'е', 'en', ' n', 'а', 'и', ' in', ' the', ' y', 'ו�', 'is', 'у', 'o', 'о', 'ом', ' с', 'er', ' в', 't', 'z', 'T', '!', ' that', 'ı', '。', ' i', \"'t\", 'י�', ' ad', 'in', ' ne', 'em', ' is']\n"
     ]
    }
   ],
   "source": [
    "freq = X_train.sum(axis=0)\n",
    "list_ids = col2ids(range(len(dict_ids)))\n",
    "list_token = [tokenizer.decode([ids]) for ids in list_ids]\n",
    "dict_freq = dict(zip(list_token,freq.tolist()[0]))\n",
    "dict_freq = dict(sorted(dict_freq.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "def ids2token(ids):\n",
    "    for token, valeur in dict_ids.items():\n",
    "        if valeur == ids:\n",
    "            token_trouvee = token\n",
    "            break\n",
    "    return token_trouvee\n",
    "\n",
    "nb_token = len(vectorizer.vocabulary_)\n",
    "print(\"Nombre de tokens :\",nb_token)\n",
    "\n",
    "# Définition d'une liste 'écrite' des tokens : decoded_keys\n",
    "# decoded_keys = [tokenizer.decode([ids]) for ids in [ids2token(key) for key in list(dict_freq.keys())]]\n",
    "decoded_keys = list(dict_freq.keys())\n",
    "print(\"Liste des 50 tokens les plus fréquents:\",decoded_keys[:50])\n",
    "\n",
    "# vocab_size = max(max(row) for row in X_train) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a96239-98ab-47b0-acf9-1660e16fc9ab",
   "metadata": {},
   "source": [
    "#### **Choix du nom du fichier de sauvegarde du classifieur** <font color='red'>(nécéssaire pour traduction texte libre)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acc778b1-231d-42c6-9218-144ee7e88da1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_file_name(titoken_tokenization, classifier):\n",
    "    return \"id_lang_tiktoken_\"+classifier+\"_sparse_big.pkl\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923233dd-f03a-400a-bc7c-b5fc4914ca8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Création d'un classificateur avec l'algorithme Naïve Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40a11f5-cf0d-4c5f-8ee1-29f6a05bf4df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "\n",
    "# On definit le classificateur Naive Bayes et on l'entraine sur les données Train:\n",
    "clf_nb = naive_bayes.MultinomialNB()  # BernoulliNB() # MultinomialNB() \n",
    "clf_nb.fit(X_train, y_train)\n",
    "\n",
    "# Save the model to a file\n",
    "# joblib.dump(clf_nb, \"../data/id_lang_tiktoken_nb_sparse_big.pkl\") ######### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54b3449f-f635-402a-b3bf-5c163fadad52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion du classificateur Naïve Bayes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Classe prédite</th>\n",
       "      <th>afr</th>\n",
       "      <th>ara</th>\n",
       "      <th>arq</th>\n",
       "      <th>asm</th>\n",
       "      <th>avk</th>\n",
       "      <th>aze</th>\n",
       "      <th>bel</th>\n",
       "      <th>ben</th>\n",
       "      <th>ber</th>\n",
       "      <th>bre</th>\n",
       "      <th>...</th>\n",
       "      <th>uig</th>\n",
       "      <th>ukr</th>\n",
       "      <th>urd</th>\n",
       "      <th>vie</th>\n",
       "      <th>vol</th>\n",
       "      <th>war</th>\n",
       "      <th>wuu</th>\n",
       "      <th>yid</th>\n",
       "      <th>yue</th>\n",
       "      <th>zsm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Classe réelle</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>afr</th>\n",
       "      <td>147</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ara</th>\n",
       "      <td>0</td>\n",
       "      <td>1901</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arq</th>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asm</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avk</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>war</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wuu</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yid</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>430</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yue</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zsm</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Classe prédite  afr   ara  arq  asm  avk  aze  bel  ben  ber  bre  ...  uig  \\\n",
       "Classe réelle                                                      ...        \n",
       "afr             147     0    0    0    0    0    0    0    0    0  ...    0   \n",
       "ara               0  1901    0    0    0    0    0    0    0    0  ...    0   \n",
       "arq               0    99    0    0    0    0    0    0    0    0  ...    1   \n",
       "asm               0     0    0  106    0    0    0   69    0    0  ...    0   \n",
       "avk               0     0    0    0  174    0    0    0    3    0  ...    0   \n",
       "...             ...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "war               0     0    0    0    0    0    0    0    0    0  ...    0   \n",
       "wuu               0     0    0    0    0    0    0    0    0    0  ...    0   \n",
       "yid               0     0    0    0    0    0    0    0    0    0  ...    0   \n",
       "yue               0     0    0    0    0    0    0    0    0    0  ...    0   \n",
       "zsm               0     7    3    0    0    0    0    0    0    0  ...    4   \n",
       "\n",
       "Classe prédite  ukr  urd  vie  vol  war  wuu  yid  yue  zsm  \n",
       "Classe réelle                                                \n",
       "afr               0    0    0    0    0    0    0    0    0  \n",
       "ara               0    0    0    0    0    0    0    0    0  \n",
       "arq               0    0    0    0    0    0    0    0    0  \n",
       "asm               0    0    0    0    0    0    0    0    0  \n",
       "avk               0    0    0    0    0    0    0    0    0  \n",
       "...             ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "war               0    0    0    0   73    0    0    0    0  \n",
       "wuu               0    0    0    0    0   75    0    2    0  \n",
       "yid               0    0    0    0    0    0  430    0    0  \n",
       "yue               0    0    0    0    0    0    0  203    0  \n",
       "zsm               0    0    0    0    0    0    0    0   95  \n",
       "\n",
       "[95 rows x 95 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Naïve Bayes = 0.960\n"
     ]
    }
   ],
   "source": [
    "# Chargement du classificateur sauvegardé\n",
    "# clf_nb = joblib.load(\"../data/id_lang_tiktoken_nb_sparse_big.pkl\")\n",
    "\n",
    "# Verification de l'efficacité du classificateur grace à la matrice confusion\n",
    "y_pred = clf_nb.predict(X_test)\n",
    "accuracy_naive_bayes = accuracy_score(y_test, y_pred)\n",
    "print(\"Matrice de confusion du classificateur Naïve Bayes\")\n",
    "ct = pd.crosstab(y_test,y_pred,rownames=['Classe réelle'], colnames=['Classe prédite'])\n",
    "display(ct)\n",
    "print(\"Accuracy Naïve Bayes = {:.3f}\".format(accuracy_naive_bayes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aefaaa37-e415-43ee-906f-7aa80ba5d79d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Naïve Bayes sur eng, deu, fran ita, spa = 0.999\n"
     ]
    }
   ],
   "source": [
    "from sklearn import naive_bayes\n",
    "ct_weurope = ct[['eng','deu','fra','ita','spa']].loc[['eng','deu','fra','ita','spa']]\n",
    "s = ct_weurope.sum().sum()\n",
    "acc = (ct_weurope.loc['eng','eng']+ct_weurope.loc['deu','deu']+ct_weurope.loc['fra','fra']+ct_weurope.loc['ita','ita']+ct_weurope.loc['spa','spa'])/s\n",
    "print(\"Accuracy Naïve Bayes sur eng, deu, fran ita, spa = {:.3f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de709b0-83c2-4b3f-a018-ea6c10531f99",
   "metadata": {},
   "source": [
    "#### **Definition de fonction identificateur de langue** <font color='red'>(nécéssaire pour traduction texte libre)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83d8d36f-3f1c-49a8-8367-38158996865c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Chargement du classificateur sauvegardé\n",
    "clf_nb = joblib.load(\"../data/id_lang_tiktoken_nb_sparse_big.pkl\")\n",
    "vectorizer = load_vectorizer()\n",
    "\n",
    "# Lisez le contenu du fichier JSON\n",
    "with open('../data/multilingue/lan_to_language.json', 'r') as fichier:\n",
    "    lan_to_language = json.load(fichier)\n",
    "\n",
    "\n",
    "\n",
    "def lang_id_nb(sentences):\n",
    "    if \"str\" in str(type(sentences)):\n",
    "        return lan_to_language[clf_nb.predict(create_BOW(sentences))[0]]\n",
    "    else: return [lan_to_language[l] for l in clf_nb.predict(create_BOW(sentences))]\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df82b0a7-3649-4c66-97d2-e38c52e2fbfc",
   "metadata": {},
   "source": [
    "#### **Exemples d'utilisation** <font color='red'>(nécéssaire pour traduction texte libre)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a73abd8-94fb-4ed5-8ad9-3308c95ca077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instanciation d'un exemple\n",
    "exemples = [\"Er weiß überhaupt nichts über dieses Buch.\",                                                             # Phrase 0\n",
    "            \"france is often snowy during spring , and it is relaxing in january .\",                                  # Phrase 1\n",
    "           \"elle adore les voitures très luxueuses, et toi ?\",                                                        # Phrase 2\n",
    "           \"she loves very luxurious cars, don't you?\",                                                               # Phrase 3\n",
    "           \"vamos a la playa\",                                                                                        # Phrase 4\n",
    "           \"Ich heiße Keyne, und das ist wunderbar\",                                                                  # Phrase 5\n",
    "           \"she loves you much, mais elle te hait aussi and das ist traurig.\", # Attention à cette phrase trilingue   # Phrase 6\n",
    "           \"A crane raises heavy construction materials.\",                                                            # Phrase 7\n",
    "           \"Vogliamo visitare il Colosseo e nuotare nel Tevere.\",                                                     # Phrase 8\n",
    "           \"私はそれについて全く知りません\"                                                                              # Phrase 9\n",
    "          ]\n",
    "lang_exemples = ['deu','eng','fra','eng','spa','deu','en,fr,de','en','ita','jpn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5b4a279-9aea-494e-b468-de0f024cd899",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langue réelle                 : ['deu', 'eng', 'fra', 'eng', 'spa', 'deu', 'en,fr,de', 'en', 'ita', 'jpn']\n",
      "Prédictions Naive Bayes       : ['German', 'English', 'French', 'English', 'Spanish', 'German', 'Galician', 'English', 'Italian', 'Japanese']\n"
     ]
    }
   ],
   "source": [
    "# Affichage des prédictions\n",
    "print('Langue réelle                 :',lang_exemples)\n",
    "print('Prédictions Naive Bayes       :',lang_id_nb(exemples))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed8d1c1-e5dd-4a17-8aee-1305fbf4c45a",
   "metadata": {},
   "source": [
    "> **Recherche des phrases mal classées par Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d458174a-c2e1-41e8-a84a-35584eaac751",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tN°Ligne\tL. réelle\tPréd. Naive B.\t\tPhrase\n",
      "1 \t 0 \t- ita \t\tItalian      \t\tIl tuo futuro è pieno di possibilità.  (proba=1.00)\n",
      "2 \t 1 \t- fra \t\tFrench       \t\tJ'aimerais aller en France, un jour.  (proba=1.00)\n",
      "3 \t 2 \t- epo \t\tEsperanto    \t\tLa polica enketo aperigis ilian sekretan vivon.  (proba=1.00)\n",
      "4 \t 3 \t- kab \t\tBerber langu \t\tKullec ifukk yid-k.  (proba=0.71)\n",
      "5 \t 4 \t- hun \t\tHungarian    \t\tTöbb munkát nem tudok elvállalni.  (proba=1.00)\n",
      "6 \t 5 \t- epo \t\tEsperanto    \t\tTiuj amikoj havas malbonan influon sur vi.  (proba=1.00)\n",
      "7 \t 6 \t- por \t\tPortuguese   \t\tSe ao menos eu soubesse!  (proba=1.00)\n",
      "8 \t 7 \t- kab \t\tBerber langu \t\tKemm d yiwet seg timeddukal n Tom, neɣ ala?  (proba=1.00)\n",
      "9 \t 8 \t- fra \t\tInterlingua  \t\tAugmente le son.  (proba=0.67)\n",
      "10 \t 9 \t- hun \t\tHungarian    \t\tOlyan keményen dolgoztam, amennyire csak lehetséges volt.  (proba=1.00)\n",
      "11 \t 10 \t- eng \t\tEnglish      \t\tHas it been proven that there's a link between tobacco and lung cancer?  (proba=1.00)\n",
      "12 \t 11 \t- fra \t\tFrench       \t\tVous devriez vous remarier.  (proba=1.00)\n",
      "13 \t 12 \t- rus \t\tRussian      \t\tДруг друга твоего отца - не обязательно друг твоего отца.  (proba=1.00)\n",
      "14 \t 13 \t- hun \t\tHungarian    \t\tEzt a bájos, orosz teremtést az Ermitázsban ismertem meg.  (proba=1.00)\n",
      "15 \t 14 \t- nld \t\tDutch        \t\tHet Engels, Russisch, Spaans en Hindi stammen af ​​van een gemeenschappelijke vooroudertaal.  (proba=1.00)\n",
      "16 \t 15 \t- tur \t\tTurkish      \t\tBaşka bir randevum var.  (proba=1.00)\n",
      "17 \t 16 \t- kab \t\tKabyle       \t\tTjerḥem-asent-tt.  (proba=0.87)\n",
      "18 \t 17 \t- epo \t\tEsperanto    \t\tGekuzoj kultivas teplantojn en sia bieno.  (proba=1.00)\n",
      "19 \t 18 \t- mai \t\tHindi        \t\tकेना छी अहाँ।  (proba=0.97)\n",
      "20 \t 19 \t- aze \t\tTurkish      \t\tOnun ana dili rus dilidir.  (proba=0.99)\n",
      "21 \t 20 \t- fin \t\tFinnish      \t\tMitä on polyamoria?  (proba=1.00)\n",
      "22 \t 21 \t- jpn \t\tJapanese     \t\tライオンはその大きな口を開けて吠えた。  (proba=1.00)\n",
      "23 \t 22 \t- rus \t\tRussian      \t\tЭто автопортрет.  (proba=1.00)\n",
      "24 \t 23 \t- ber \t\tBerber langu \t\tTom yebɣa ad yessulli.  (proba=0.98)\n",
      "25 \t 24 \t- tur \t\tTurkish      \t\tTom romantik.  (proba=0.70)\n",
      "26 \t 25 \t- kab \t\tKabyle       \t\tAmek dɣa ara kettbent azemz s tlatinit?  (proba=1.00)\n",
      "27 \t 26 \t- tur \t\tTurkish      \t\tPeşimizde kimin olduğunu söyle bize.  (proba=1.00)\n",
      "28 \t 27 \t- por \t\tPortuguese   \t\tEu jamais faria isso por elas.  (proba=1.00)\n",
      "29 \t 28 \t- hun \t\tHungarian    \t\tVan valami, amit el akarok neked mondani.  (proba=1.00)\n",
      "30 \t 29 \t- ita \t\tItalian      \t\tDice di non essere interessata.  (proba=1.00)\n"
     ]
    }
   ],
   "source": [
    "n_bad_max = 30\n",
    "n_bad = 0\n",
    "print(\"\\tN°Ligne\\tL. réelle\\tPréd. Naive B.\\t\\tPhrase\")\n",
    "for i in range(len(df)):\n",
    "    if (n_bad<n_bad_max):\n",
    "        if (df['lan_code'].iloc[i] != lang_id_nb(df['sentence'].iloc[i])):\n",
    "            n_bad +=1\n",
    "            print(n_bad,'\\t',i,'\\t-',df['lan_code'].iloc[i],'\\t\\t'+lang_id_nb(df['sentence'].iloc[i]).ljust(12)[:12],'\\t\\t'+\n",
    "                  df['sentence'].iloc[i],\" (proba={:.2f}\".format(max(clf_nb.predict_proba(create_BOW([df['sentence'].iloc[i]]))[0]))+\")\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
